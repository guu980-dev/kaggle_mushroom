{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-28 19:41:54.830541: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-28 19:41:54.839165: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-28 19:41:54.849260: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-28 19:41:54.852310: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-28 19:41:54.860068: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-28 19:41:55.352350: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tensorflow import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#   project='kaggle_mushroom',\n",
    "#   config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#   }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/playground-series-s4e8/train.csv\", index_col='id')\n",
    "orig_df = pd.read_csv(\"data/secondary-mushroom-dataset-data-set/MushroomDataset/secondary_data.csv\", sep=\";\")\n",
    "test_df = pd.read_csv(\"data/playground-series-s4e8/test.csv\", index_col='id')\n",
    "train_df = pd.concat([train_df, orig_df], ignore_index=True) # Combine the competition data with the secondary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Categorize Features\n",
    "target = 'class'\n",
    "features = train_df.drop(target, axis=1).columns.to_list()\n",
    "features_with_high_null_values = [feature for feature in features if (train_df[feature].isna().sum()/len(train_df)*100)>20]\n",
    "categorical_features = train_df[features].select_dtypes(include='object').columns.to_list()\n",
    "numerical_features = list(set(features) - set(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Impute Missing Values (Null Values) '''\n",
    "\n",
    "# Clean Categorical Features\n",
    "def cleaner(df):\n",
    "    for col in categorical_features:\n",
    "        df[col] = df[col].fillna('missing')\n",
    "        df.loc[df[col].value_counts(dropna=False)[df[col]].values < 100, col] = \"noise\"\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = cleaner(train_df)\n",
    "test_df = cleaner(test_df)\n",
    "\n",
    "# Clean Numerical Features with mean\n",
    "cap_diameter_mean = pd.concat([train_df['cap-diameter'], test_df['cap-diameter']]).mean(numeric_only=True)\n",
    "train_df['cap-diameter'].fillna(cap_diameter_mean, inplace=True)\n",
    "test_df['cap-diameter'].fillna(cap_diameter_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.copy()\n",
    "y = X.pop(target)\n",
    "\n",
    "lab_enc = LabelEncoder().fit(y)\n",
    "y = lab_enc.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(estimator, X, y, cv=5):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"    Model: {estimator.__class__.__name__}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1/cv, shuffle=True, stratify=y, random_state=42)\n",
    "    \n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    print(f\"F1 Score : {f1.mean():.6f}\")\n",
    "    print(f\"MCC Score: {mcc.mean():.6f}\")\n",
    "    \n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(model, X, y, test_set, skfold):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    print(\"=\"*80, end=\"\\n\")\n",
    "\n",
    "    oof_mccs = []\n",
    "    test_probs = np.zeros((test_set.shape[0],))   # test set probabilities. Average of each fold's probability prediction on test set.\n",
    "    oof_probs = np.zeros((X.shape[0],))   # training set probabilities\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y)):\n",
    "        # Select train, validation set by each fold\n",
    "        X_train, y_train = X.iloc[train_idx, :], y[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx, :], y[val_idx]\n",
    "\n",
    "        # Train Model\n",
    "        model = clone(model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate by validation set\n",
    "        val_probs = model.predict_proba(X_val)[:, 1]   # Calculate probability\n",
    "        oof_probs[val_idx] = val_probs   # Save probability of this fold's validation set\n",
    "        val_preds = (val_probs > 0.5).astype(int)   # Convert probability to label\n",
    "        mcc = matthews_corrcoef(y_val, val_preds)   # Calculate mcc score\n",
    "        oof_mccs.append(mcc)   # Save mcc score of this fold's validation set\n",
    "        print(f\"--- Fold {fold+1} MCC Score: {mcc:.6f}\")\n",
    "\n",
    "        # Predict on test set\n",
    "        test_probs += model.predict_proba(test_set)[:, 1] / skfold.get_n_splits()   # Aggregate test set probabilities\n",
    "\n",
    "    mean_mcc_score = np.mean(oof_mccs)\n",
    "    std_mcc_score = np.std(oof_mccs)\n",
    "    print(f\"\\n---> Mean MCC Score: {mean_mcc_score:.6f} \\xb1 {std_mcc_score:.6f}\\n\\n\")\n",
    "    \n",
    "    total_mcc_score = matthews_corrcoef(y, (oof_probs > 0.5).astype(int))\n",
    "    print(f\"Total MCC Score: {total_mcc_score:.6f}\")\n",
    "\n",
    "    return oof_mccs, test_probs, oof_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' tree-based models '''\n",
    "\n",
    "xgb_clf = XGBClassifier(enable_categorical=True, device=\"cuda\", tree_method=\"hist\")\n",
    "cat_clf = CatBoostClassifier(\n",
    "    cat_features=categorical_features,\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    "    task_type=\"GPU\"\n",
    ")\n",
    "lgb_clf = LGBMClassifier(device='gpu', verbosity=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724841731.921709   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841731.945368   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841731.946740   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841731.948599   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841731.949807   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841731.951012   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841732.028522   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841732.029381   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724841732.030182   25478 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-28 19:42:12.030948: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9519 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Neural Network models '''\n",
    "\n",
    "# Neural network (Keras) with two hidden layers (one-hot encoded)\n",
    "def get_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(512, activation='linear'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.36))\n",
    "    model.add(keras.layers.Dense(256, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.3))\n",
    "    model.add(keras.layers.Dense(128, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.24))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.2))\n",
    "    model.add(keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.16))\n",
    "    model.add(keras.layers.Dense(16, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dropout(0.12))\n",
    "    model.add(keras.layers.Dense(8, activation='relu'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "display(keras.utils.plot_model(get_model({\"X_shape_\": X.shape}),\n",
    "                       show_shapes=True, show_layer_activations=True, dpi=80))\n",
    "\n",
    "\n",
    "nn_clf = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.01),\n",
    "    validation_split=0.03,\n",
    "    batch_size=8192,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=53,\n",
    "    # verbose=0, # or 2\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3),\n",
    "               keras.callbacks.EarlyStopping(patience=5, min_delta=0.00003)]\n",
    ")\n",
    "\n",
    "nn_model = make_pipeline(ColumnTransformer([\n",
    "                                        ('float', make_pipeline(PowerTransformer(),\n",
    "                                                                 SimpleImputer(add_indicator=True)),\n",
    "                                          numerical_features),\n",
    "                                         ('cat', OneHotEncoder(drop='first',\n",
    "                                                               sparse_output=False,\n",
    "                                                               dtype=np.float32,\n",
    "                                                               handle_unknown='infrequent_if_exist',\n",
    "                                                               min_frequency=50),\n",
    "                                          categorical_features),\n",
    "                                        ]),\n",
    "                      StandardScaler(),\n",
    "                      BaggingClassifier(nn_clf, n_estimators=7, bootstrap=False))\n",
    "# cross_validate(nn_model, 'Keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 2407,\n",
    "    'eta': 0.009462133032592785,\n",
    "    'gamma': 0.2865859948765318,\n",
    "    'max_depth': 31,\n",
    "    'min_child_weight': 47,\n",
    "    'subsample': 0.6956431754146083,\n",
    "    'colsample_bytree': 0.3670732604094118,\n",
    "    'grow_policy': 'lossguide',\n",
    "    'max_leaves': 73,\n",
    "    'enable_categorical': True,\n",
    "    'n_jobs': -1,\n",
    "    'device': 'cuda',\n",
    "    'tree_method': 'hist'\n",
    "} # 0.9844272567086021\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 1041,\n",
    "    'learning_rate': 0.08777255350163136,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 0.1259643500248322,\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'random_strength': 4.276181166674371e-08,\n",
    "    'bagging_temperature': 0.35995482350907326,\n",
    "    'od_type': 'Iter',\n",
    "    'od_wait': 39,\n",
    "    \"verbose\": False,\n",
    "    \"allow_writing_files\": False,\n",
    "    \"task_type\": 'GPU',\n",
    "    \"cat_features\": categorical_features\n",
    "} # 0.9841773055825763\n",
    "\n",
    "lgb_params = {\n",
    "    'n_estimators': 2500,\n",
    "    'random_state':42,\n",
    "    'max_bin':1024,\n",
    "    'colsample_bytree':0.6,\n",
    "    'reg_lambda': 80,\n",
    "    # 'device': 'gpu',\n",
    "    'verbosity': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% time\n",
    "cv_summary, test_probs, oof_probs = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "random_state = 101\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training XGBClassifier\n",
      "================================================================================\n",
      "--- Fold 1 MCC Score: 0.984879\n",
      "--- Fold 2 MCC Score: 0.984927\n",
      "--- Fold 3 MCC Score: 0.984749\n",
      "--- Fold 4 MCC Score: 0.984930\n",
      "--- Fold 5 MCC Score: 0.984882\n",
      "\n",
      "---> Mean MCC Score: 0.984873 ± 0.000066\n",
      "\n",
      "\n",
      "Total MCC Score: 0.984873\n"
     ]
    }
   ],
   "source": [
    "cv_summary['xgb'], test_probs['xgb'], oof_probs['xgb'] = model_trainer(XGBClassifier(**xgb_params), X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training CatBoostClassifier\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 1489.375 Total: 11901.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 1 MCC Score: 0.984749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 1636.125 Total: 11901.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 2 MCC Score: 0.984860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 1644.125 Total: 11901.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 3 MCC Score: 0.984470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 1581.6875 Total: 11901.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 4 MCC Score: 0.984838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: less than 75% gpu memory available for training. Free: 1676.5 Total: 11901.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Fold 5 MCC Score: 0.984675\n",
      "\n",
      "---> Mean MCC Score: 0.984718 ± 0.000140\n",
      "\n",
      "\n",
      "Total MCC Score: 0.984718\n"
     ]
    }
   ],
   "source": [
    "cv_summary['cat'], test_probs['cat'], oof_probs['cat'] = model_trainer(CatBoostClassifier(**cat_params), X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training LGBMClassifier\n",
      "================================================================================\n",
      "--- Fold 1 MCC Score: 0.985025\n",
      "--- Fold 2 MCC Score: 0.984989\n",
      "--- Fold 3 MCC Score: 0.984879\n",
      "--- Fold 4 MCC Score: 0.984977\n",
      "--- Fold 5 MCC Score: 0.984971\n",
      "\n",
      "---> Mean MCC Score: 0.984968 ± 0.000048\n",
      "\n",
      "\n",
      "Total MCC Score: 0.984968\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cv_summary['lgb'], test_probs['lgb'], oof_probs['lgb'] = model_trainer(LGBMClassifier(**lgb_params), X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Pipeline\n",
      "================================================================================\n",
      "Epoch 1/53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724843653.395851   26001 service.cc:146] XLA service 0x7116b8013f50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724843653.395870   26001 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti, Compute Capability 8.9\n",
      "2024-08-28 20:14:13.459836: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-28 20:14:13.788307: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 42/302\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.3602"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1724843655.995440   26001 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.1359 - val_loss: 0.0247 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0481 - val_loss: 0.0223 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0460 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0439 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0432 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0429 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0424 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0421 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0410 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0401 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0396 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0167 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0167 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0167 - learning_rate: 1.0000e-05\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0167 - learning_rate: 1.0000e-05\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0167 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1468 - val_loss: 0.0237 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0492 - val_loss: 0.0211 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0455 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0444 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0436 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0433 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0426 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0426 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0422 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0413 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0405 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.1350 - val_loss: 0.0207 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0484 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0459 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0443 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0439 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0433 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0428 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0414 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1470 - val_loss: 0.0227 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0480 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0459 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0445 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0439 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0422 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0413 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0410 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0407 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0407 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0406 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0405 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0399 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0168 - learning_rate: 1.0000e-05\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0168 - learning_rate: 1.0000e-05\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0168 - learning_rate: 1.0000e-05\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0168 - learning_rate: 1.0000e-06\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.1423 - val_loss: 0.0360 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0508 - val_loss: 0.0232 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0465 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0441 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0433 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0434 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0431 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0425 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0434 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0426 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0422 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0420 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0406 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1372 - val_loss: 0.0317 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0500 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0462 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0447 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0440 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0431 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0429 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0423 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0426 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0424 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0418 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0409 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0396 - val_loss: 0.0165 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0165 - learning_rate: 1.0000e-03\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0165 - learning_rate: 1.0000e-04\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0164 - learning_rate: 1.0000e-04\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0390 - val_loss: 0.0165 - learning_rate: 1.0000e-04\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0390 - val_loss: 0.0165 - learning_rate: 1.0000e-05\n",
      "Epoch 27/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0391 - val_loss: 0.0165 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.1322 - val_loss: 0.0256 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0490 - val_loss: 0.0208 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0455 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0442 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0436 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0432 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0429 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0427 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0421 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0415 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0407 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "--- Fold 1 MCC Score: 0.984126\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1371 - val_loss: 0.0287 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0485 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0460 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0443 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0440 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0430 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0425 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0412 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0400 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0175 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 11ms/step - loss: 0.1450 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0480 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0454 - val_loss: 0.0222 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0442 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0428 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0429 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0421 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0421 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0417 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0417 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0404 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0396 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0395 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0391 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0390 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0390 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0389 - val_loss: 0.0168 - learning_rate: 1.0000e-05\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0390 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 27/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0390 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 28/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0389 - val_loss: 0.0169 - learning_rate: 1.0000e-06\n",
      "Epoch 29/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0389 - val_loss: 0.0169 - learning_rate: 1.0000e-06\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.1314 - val_loss: 0.0226 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0493 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0471 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0450 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0444 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0438 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0437 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0429 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0432 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0423 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0419 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0409 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1456 - val_loss: 0.0241 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0486 - val_loss: 0.0208 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0453 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0439 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0438 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0426 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0425 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0418 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0417 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0416 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0415 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0414 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0414 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0411 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0410 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0409 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1410 - val_loss: 0.0251 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0477 - val_loss: 0.0241 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0460 - val_loss: 0.0219 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0448 - val_loss: 0.0207 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0434 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0431 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0429 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0426 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0421 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0414 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0403 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0400 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0395 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0389 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0389 - val_loss: 0.0168 - learning_rate: 1.0000e-05\n",
      "Epoch 27/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0388 - val_loss: 0.0168 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1408 - val_loss: 0.0251 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0475 - val_loss: 0.0257 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0452 - val_loss: 0.0267 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0446 - val_loss: 0.0242 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0427 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0424 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0417 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0408 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1373 - val_loss: 0.0230 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0482 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0450 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0440 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0436 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0427 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0426 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0424 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0419 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0417 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0409 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0403 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0396 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - loss: 0.0394 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0393 - val_loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0392 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0389 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "--- Fold 2 MCC Score: 0.984495\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.1398 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0479 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0455 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0443 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0433 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0429 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0424 - val_loss: 0.0167 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0417 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0416 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0408 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0402 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.1472 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0481 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0457 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0443 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0432 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0427 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0422 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0410 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0403 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0401 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0399 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0394 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0394 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0392 - val_loss: 0.0165 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0392 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0165 - learning_rate: 1.0000e-05\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0391 - val_loss: 0.0165 - learning_rate: 1.0000e-05\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0165 - learning_rate: 1.0000e-05\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0391 - val_loss: 0.0165 - learning_rate: 1.0000e-06\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.1428 - val_loss: 0.0240 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0474 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0462 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0438 - val_loss: 0.0223 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0431 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0430 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0425 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0422 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0421 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0408 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0400 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0397 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0395 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0392 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0392 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0167 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0388 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0388 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0387 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0387 - val_loss: 0.0166 - learning_rate: 1.0000e-05\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0388 - val_loss: 0.0166 - learning_rate: 1.0000e-05\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0387 - val_loss: 0.0166 - learning_rate: 1.0000e-05\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0386 - val_loss: 0.0166 - learning_rate: 1.0000e-06\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0387 - val_loss: 0.0166 - learning_rate: 1.0000e-06\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1382 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0476 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0453 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0441 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0435 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0432 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0427 - val_loss: 0.0165 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0425 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0424 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0410 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.1474 - val_loss: 0.0225 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0480 - val_loss: 0.0219 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0457 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0445 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0436 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0432 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0428 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0164 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0160 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0416 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0415 - val_loss: 0.0164 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0414 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0405 - val_loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0399 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.1425 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0480 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0453 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0438 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0436 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0427 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0431 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0419 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0419 - val_loss: 0.0164 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0417 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0414 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0414 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0403 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0396 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 15ms/step - loss: 0.1380 - val_loss: 0.0249 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0477 - val_loss: 0.0230 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0456 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0438 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0438 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0430 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0430 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0414 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0406 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0404 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0401 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0400 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0165 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0397 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0396 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0395 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0394 - val_loss: 0.0165 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0164 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0389 - val_loss: 0.0164 - learning_rate: 1.0000e-04\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "--- Fold 3 MCC Score: 0.983919\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1332 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0478 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0455 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0446 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0433 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0434 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0426 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0425 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0421 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0419 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0416 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0405 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0390 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0388 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0388 - val_loss: 0.0171 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 14ms/step - loss: 0.1349 - val_loss: 0.0227 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0471 - val_loss: 0.0230 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0455 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0436 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0435 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0426 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0422 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0419 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0415 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0416 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0413 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0412 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1358 - val_loss: 0.0243 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0474 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0448 - val_loss: 0.0217 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0438 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0433 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0424 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0421 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0419 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0407 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1390 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0476 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0456 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0441 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0431 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0431 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0423 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0419 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0416 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0415 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0405 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1435 - val_loss: 0.0233 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0502 - val_loss: 0.0209 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0453 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0447 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0446 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0432 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0429 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0428 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0415 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0407 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0399 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0400 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0398 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0396 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0396 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0395 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0395 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0392 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0394 - val_loss: 0.0169 - learning_rate: 1.0000e-06\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0169 - learning_rate: 1.0000e-06\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1411 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0487 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0462 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0447 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0443 - val_loss: 0.0203 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0436 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0427 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0433 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0421 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0420 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0410 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0403 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1356 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0494 - val_loss: 0.0215 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0456 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0445 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0439 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0435 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0432 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0430 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0426 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0426 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0420 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0417 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0416 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0408 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0401 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "--- Fold 4 MCC Score: 0.984214\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1384 - val_loss: 0.0221 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0477 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0454 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0444 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0437 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0433 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0429 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0426 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0419 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0422 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0424 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0417 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0416 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0412 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0415 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0413 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0404 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1367 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0478 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0458 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0440 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0434 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0430 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0414 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0402 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0399 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0397 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 12ms/step - loss: 0.1375 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0489 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0458 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0442 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0443 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0434 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0439 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0418 - val_loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0409 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0408 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0175 - learning_rate: 1.0000e-04\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0403 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0403 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0403 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0174 - learning_rate: 1.0000e-06\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0402 - val_loss: 0.0174 - learning_rate: 1.0000e-06\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0174 - learning_rate: 1.0000e-06\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0404 - val_loss: 0.0174 - learning_rate: 1.0000e-07\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1374 - val_loss: 0.0228 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0469 - val_loss: 0.0218 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0450 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0442 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0431 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0421 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0419 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0417 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0400 - val_loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 13ms/step - loss: 0.1542 - val_loss: 0.0261 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0489 - val_loss: 0.0209 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0460 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0448 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0439 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0431 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0425 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0423 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0419 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0407 - val_loss: 0.0180 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0400 - val_loss: 0.0179 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0396 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0396 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0395 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0393 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0392 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0390 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - loss: 0.1447 - val_loss: 0.0217 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0474 - val_loss: 0.0220 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0456 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0444 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0445 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0434 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0428 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0427 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0420 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0418 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0416 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0401 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 13ms/step - loss: 0.1428 - val_loss: 0.0244 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0492 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0458 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0441 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0435 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0436 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0420 - val_loss: 0.0178 - learning_rate: 1.0000e-03\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0409 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0404 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0403 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0401 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0400 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0171 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0171 - learning_rate: 1.0000e-05\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0397 - val_loss: 0.0172 - learning_rate: 1.0000e-05\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0398 - val_loss: 0.0172 - learning_rate: 1.0000e-05\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "--- Fold 5 MCC Score: 0.983971\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "---> Mean MCC Score: 0.984145 ± 0.000204\n",
      "\n",
      "\n",
      "Total MCC Score: 0.984145\n"
     ]
    }
   ],
   "source": [
    "cv_summary['nn'], test_probs['nn'], oof_probs['nn'] = model_trainer(nn_model, X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_771b9_row0_col0, #T_771b9_row0_col1, #T_771b9_row0_col2, #T_771b9_row0_col3, #T_771b9_row0_col4, #T_771b9_row0_col5, #T_771b9_row1_col3, #T_771b9_row1_col4, #T_771b9_row1_col5, #T_771b9_row3_col6 {\n",
       "  background-color: #1b9e77;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_771b9_row0_col6, #T_771b9_row1_col6, #T_771b9_row3_col0, #T_771b9_row3_col1, #T_771b9_row3_col2, #T_771b9_row3_col3, #T_771b9_row3_col4, #T_771b9_row3_col5 {\n",
       "  background-color: #666666;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_771b9_row1_col0, #T_771b9_row1_col1, #T_771b9_row1_col2, #T_771b9_row2_col3 {\n",
       "  background-color: #d95f02;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_771b9_row2_col0, #T_771b9_row2_col1, #T_771b9_row2_col4, #T_771b9_row2_col5 {\n",
       "  background-color: #7570b3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_771b9_row2_col2, #T_771b9_row2_col6 {\n",
       "  background-color: #e7298a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_771b9\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_771b9_level0_col0\" class=\"col_heading level0 col0\" >fold1</th>\n",
       "      <th id=\"T_771b9_level0_col1\" class=\"col_heading level0 col1\" >fold2</th>\n",
       "      <th id=\"T_771b9_level0_col2\" class=\"col_heading level0 col2\" >fold3</th>\n",
       "      <th id=\"T_771b9_level0_col3\" class=\"col_heading level0 col3\" >fold4</th>\n",
       "      <th id=\"T_771b9_level0_col4\" class=\"col_heading level0 col4\" >fold5</th>\n",
       "      <th id=\"T_771b9_level0_col5\" class=\"col_heading level0 col5\" >Mean</th>\n",
       "      <th id=\"T_771b9_level0_col6\" class=\"col_heading level0 col6\" >Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row0\" class=\"row_heading level0 row0\" >lgb</th>\n",
       "      <td id=\"T_771b9_row0_col0\" class=\"data row0 col0\" >0.985025</td>\n",
       "      <td id=\"T_771b9_row0_col1\" class=\"data row0 col1\" >0.984989</td>\n",
       "      <td id=\"T_771b9_row0_col2\" class=\"data row0 col2\" >0.984879</td>\n",
       "      <td id=\"T_771b9_row0_col3\" class=\"data row0 col3\" >0.984977</td>\n",
       "      <td id=\"T_771b9_row0_col4\" class=\"data row0 col4\" >0.984971</td>\n",
       "      <td id=\"T_771b9_row0_col5\" class=\"data row0 col5\" >0.984968</td>\n",
       "      <td id=\"T_771b9_row0_col6\" class=\"data row0 col6\" >0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row1\" class=\"row_heading level0 row1\" >xgb</th>\n",
       "      <td id=\"T_771b9_row1_col0\" class=\"data row1 col0\" >0.984879</td>\n",
       "      <td id=\"T_771b9_row1_col1\" class=\"data row1 col1\" >0.984927</td>\n",
       "      <td id=\"T_771b9_row1_col2\" class=\"data row1 col2\" >0.984749</td>\n",
       "      <td id=\"T_771b9_row1_col3\" class=\"data row1 col3\" >0.984930</td>\n",
       "      <td id=\"T_771b9_row1_col4\" class=\"data row1 col4\" >0.984882</td>\n",
       "      <td id=\"T_771b9_row1_col5\" class=\"data row1 col5\" >0.984873</td>\n",
       "      <td id=\"T_771b9_row1_col6\" class=\"data row1 col6\" >0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row2\" class=\"row_heading level0 row2\" >cat</th>\n",
       "      <td id=\"T_771b9_row2_col0\" class=\"data row2 col0\" >0.984749</td>\n",
       "      <td id=\"T_771b9_row2_col1\" class=\"data row2 col1\" >0.984860</td>\n",
       "      <td id=\"T_771b9_row2_col2\" class=\"data row2 col2\" >0.984470</td>\n",
       "      <td id=\"T_771b9_row2_col3\" class=\"data row2 col3\" >0.984838</td>\n",
       "      <td id=\"T_771b9_row2_col4\" class=\"data row2 col4\" >0.984675</td>\n",
       "      <td id=\"T_771b9_row2_col5\" class=\"data row2 col5\" >0.984718</td>\n",
       "      <td id=\"T_771b9_row2_col6\" class=\"data row2 col6\" >0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_771b9_level0_row3\" class=\"row_heading level0 row3\" >nn</th>\n",
       "      <td id=\"T_771b9_row3_col0\" class=\"data row3 col0\" >0.984126</td>\n",
       "      <td id=\"T_771b9_row3_col1\" class=\"data row3 col1\" >0.984495</td>\n",
       "      <td id=\"T_771b9_row3_col2\" class=\"data row3 col2\" >0.983919</td>\n",
       "      <td id=\"T_771b9_row3_col3\" class=\"data row3 col3\" >0.984214</td>\n",
       "      <td id=\"T_771b9_row3_col4\" class=\"data row3 col4\" >0.983971</td>\n",
       "      <td id=\"T_771b9_row3_col5\" class=\"data row3 col5\" >0.984145</td>\n",
       "      <td id=\"T_771b9_row3_col6\" class=\"data row3 col6\" >0.000204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x71171b68bac0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance Summary for comparing model (Not Needed)\n",
    "transposed_df = cv_summary.transpose()\n",
    "transposed_df.columns = ['fold1','fold2','fold3','fold4','fold5']\n",
    "transposed_df['Mean'] = transposed_df.mean(axis=1)\n",
    "transposed_df['Std'] = transposed_df.std(axis=1)\n",
    "transposed_df.sort_values(by = 'Mean', ascending=False).style.background_gradient('Dark2_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model diversity check (Not Needed)\n",
    "sns.set(font_scale=1.1)\n",
    "correlation_train = oof_probs.corr()\n",
    "mask = np.triu(correlation_train.corr())\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_train,\n",
    "            annot=True,\n",
    "            fmt='.3f',\n",
    "            cmap='coolwarm',\n",
    "            square=True,\n",
    "            mask=mask,\n",
    "            linewidths=1,\n",
    "            cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting hard & soft\n",
    "def voting_ensemble(oof_probs, y, threshold=0.5, voting_type='soft'):\n",
    "    if voting_type == 'soft':\n",
    "        ensemble_preds = oof_probs.mean(axis=1)\n",
    "        ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n",
    "        \n",
    "    elif voting_type == 'hard':\n",
    "        binary_preds = (oof_probs > threshold).astype(int)\n",
    "        ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n",
    "    \n",
    "    mcc_score = matthews_corrcoef(y, ensemble_class_preds)\n",
    "    \n",
    "    return mcc_score, ensemble_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9851378386766322\n"
     ]
    }
   ],
   "source": [
    "soft_vote_score, soft_vote_pred = voting_ensemble(oof_probs, y, voting_type='soft')\n",
    "print(soft_vote_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850263211810737\n"
     ]
    }
   ],
   "source": [
    "hard_vote_score, hard_vote_pred = voting_ensemble(oof_probs, y, voting_type='hard')\n",
    "print(hard_vote_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting hard & soft\n",
    "def voting_ensemble_only_pred(oof_probs, threshold=0.5, voting_type='soft'):\n",
    "  if voting_type == 'soft':\n",
    "      ensemble_preds = oof_probs.mean(axis=1)\n",
    "      ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n",
    "      \n",
    "  elif voting_type == 'hard':\n",
    "      binary_preds = (oof_probs > threshold).astype(int)\n",
    "      ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n",
    "\n",
    "  return ensemble_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_vote_pred_test = voting_ensemble_only_pred(test_probs, voting_type='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          1\n",
      "2          1\n",
      "3          1\n",
      "4          0\n",
      "          ..\n",
      "2077959    1\n",
      "2077960    1\n",
      "2077961    1\n",
      "2077962    0\n",
      "2077963    0\n",
      "Length: 2077964, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(soft_vote_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for meta model                                                                                                 \n",
    "meta_model_params = {\n",
    "    'C': 0.000237302749626327,\n",
    "    'max_iter': 2500,\n",
    "    'tol': 9.996751434702547e-05,\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l1'\n",
    "}\n",
    "\n",
    "meta_model = LogisticRegression(**meta_model_params, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: \n",
      "0.9851016234744895\n",
      "Number of available models: 4\n",
      "Number of selected models for ensemble: 4\n",
      "Selected models: ['xgb' 'cat' 'lgb' 'nn']\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "#Deciding which models to include ensemble\n",
    "\n",
    "min_features_to_select = 1\n",
    "\n",
    "# Create a pipeline with preprocessor and RFECV\n",
    "pipeline = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('rfecv', RFECV(estimator=meta_model,\n",
    "                    step=1,\n",
    "                    cv=skfold,\n",
    "                    scoring=make_scorer(matthews_corrcoef),\n",
    "                    min_features_to_select=min_features_to_select,\n",
    "                    n_jobs=-1,))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on oof_preds\n",
    "pipeline.fit(oof_probs, y)\n",
    "\n",
    "#CV score\n",
    "print(\"Best CV score: \")\n",
    "selected_models = np.array(oof_probs.columns)[pipeline.named_steps['rfecv'].support_]\n",
    "print( pipeline.named_steps['rfecv'].cv_results_[\"mean_test_score\"][len(selected_models) - 1])\n",
    "\n",
    "\n",
    "# Selected models after RFECV\n",
    "print('Number of available models:', len(oof_probs.columns))\n",
    "print('Number of selected models for ensemble:', len(selected_models))\n",
    "print(\"Selected models:\", selected_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = meta_model.fit(oof_probs[selected_models], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985101016973319\n"
     ]
    }
   ],
   "source": [
    "preds_train =  meta_model.predict(oof_probs[selected_models])\n",
    "mcc_score = matthews_corrcoef(y, preds_train)\n",
    "print(mcc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test =  meta_model.predict(test_probs[selected_models])\n",
    "preds_test = lab_enc.inverse_transform(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Intermediate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_summary['lgb'], test_probs['lgb'], oof_probs['lgb']\n",
    "exp_name = \"deep_nn_test1\"\n",
    "\n",
    "from pathlib import Path\n",
    "Path(f\"result/{exp_name}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cv_summary.to_pickle(f\"result/{exp_name}/cv_summary.pkl\")\n",
    "test_probs.to_pickle(f\"result/{exp_name}/test_probs.pkl\")\n",
    "oof_probs.to_pickle(f\"result/{exp_name}/oof_probs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Stacking '''\n",
    "\n",
    "output = pd.DataFrame({'id': test_df.index,\n",
    "                       'class': preds_test})\n",
    "output.to_csv('pred/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Soft Voting '''\n",
    "# soft_vote_pred_test = lab_enc.inverse_transform(soft_vote_pred_test)\n",
    "# output = pd.DataFrame({'id': test_df.index,\n",
    "#                        'class': soft_vote_pred_test})\n",
    "# output.to_csv('pred/submission_soft_vote_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save oofs and test predictions for later usage\n",
    "# oof_probs.to_parquet('oof_predictions_v01.parquet', index=False)\n",
    "# test_probs.to_parquet('test_predictions_v01.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(\"/data/playground-series-s4e8/sample_submission.csv\")\n",
    "# preds = [pred for model, pred in oof_preds.items()]\n",
    "# md = mode(preds, axis=0)[0] if len(preds)>1 else preds[0]\n",
    "# sub[target] = lab_enc.inverse_transform(md)\n",
    "# sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext1 = pd.read_csv(\"/kaggle/input/mario-s-nightmare-15-th-place-solution/submission.csv\")[target].ravel()\n",
    "# ext2 = pd.read_csv(\"/kaggle/input/ps4e8-binary-class-mathews-correlation-coeff/submission.csv\")[target].ravel()\n",
    "# ext3 = pd.read_csv(\"/kaggle/input/playgrounds4e08-modeblend/submission.csv\")[target].ravel()\n",
    "# ext4 = pd.read_csv(\"/kaggle/input/autogloun-t8-dslanders/submission.csv\")[target].ravel()\n",
    "# ext5 = pd.read_csv(\"/kaggle/input/mario-s-nightmare-denselight-0-990/submission_test7.csv\")[target].ravel()\n",
    "\n",
    "# preds = [ext1, ext2, ext3, ext4, ext5]\n",
    "# preds = [lab_enc.transform(x) for x in preds]\n",
    "# md = mode(preds, axis=0)[0]\n",
    "# sub[target] = lab_enc.inverse_transform(md)\n",
    "# sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
