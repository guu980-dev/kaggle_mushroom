{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-29 16:03:42.888025: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-29 16:03:43.106092: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-29 16:03:43.194803: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-29 16:03:43.214536: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-29 16:03:43.335544: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-29 16:03:44.068438: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.metrics import matthews_corrcoef, f1_score, confusion_matrix, ConfusionMatrixDisplay, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from scipy.stats import mode\n",
    "from sklearn.base import clone\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, PowerTransformer\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from tensorflow import keras\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.init(\n",
    "#   project='kaggle_mushroom',\n",
    "#   config={\n",
    "#     \"learning_rate\": 0.02,\n",
    "#     \"architecture\": \"CNN\",\n",
    "#     \"dataset\": \"CIFAR-100\",\n",
    "#     \"epochs\": 10,\n",
    "#   }\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/playground-series-s4e8/train.csv\", index_col='id')\n",
    "orig_df = pd.read_csv(\"data/secondary-mushroom-dataset-data-set/MushroomDataset/secondary_data.csv\", sep=\";\")\n",
    "test_df = pd.read_csv(\"data/playground-series-s4e8/test.csv\", index_col='id')\n",
    "train_df = pd.concat([train_df, orig_df], ignore_index=True) # Combine the competition data with the secondary data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicates\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Categorize Features\n",
    "target = 'class'\n",
    "features = train_df.drop(target, axis=1).columns.to_list()\n",
    "features_with_high_null_values = [feature for feature in features if (train_df[feature].isna().sum()/len(train_df)*100)>20]\n",
    "categorical_features = train_df[features].select_dtypes(include='object').columns.to_list()\n",
    "numerical_features = list(set(features) - set(categorical_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Impute Missing Values (Null Values) '''\n",
    "\n",
    "# Clean Categorical Features\n",
    "def cleaner(df):\n",
    "    for col in categorical_features:\n",
    "        df[col] = df[col].fillna('missing')\n",
    "        df.loc[df[col].value_counts(dropna=False)[df[col]].values < 100, col] = \"noise\"\n",
    "        df[col] = df[col].astype('category')\n",
    "\n",
    "    return df\n",
    "\n",
    "train_df = cleaner(train_df)\n",
    "test_df = cleaner(test_df)\n",
    "\n",
    "# Clean Numerical Features with mean\n",
    "cap_diameter_mean = pd.concat([train_df['cap-diameter'], test_df['cap-diameter']]).mean(numeric_only=True)\n",
    "train_df['cap-diameter'].fillna(cap_diameter_mean, inplace=True)\n",
    "test_df['cap-diameter'].fillna(cap_diameter_mean, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.copy()\n",
    "y = X.pop(target)\n",
    "\n",
    "lab_enc = LabelEncoder().fit(y)\n",
    "y = lab_enc.transform(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_report(estimator, X, y, cv=5):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"    Model: {estimator.__class__.__name__}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=1/cv, shuffle=True, stratify=y, random_state=42)\n",
    "    \n",
    "    estimator.fit(X_train, y_train)\n",
    "    y_pred = estimator.predict(X_test)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    print(f\"F1 Score : {f1.mean():.6f}\")\n",
    "    print(f\"MCC Score: {mcc.mean():.6f}\")\n",
    "    \n",
    "    ConfusionMatrixDisplay(confusion_matrix(y_test, y_pred)).plot()\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_trainer(model, X, y, test_set, skfold):\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Training {model.__class__.__name__}\")\n",
    "    print(\"=\"*80, end=\"\\n\")\n",
    "\n",
    "    oof_mccs = []\n",
    "    test_probs = np.zeros((test_set.shape[0],))   # test set probabilities. Average of each fold's probability prediction on test set.\n",
    "    oof_probs = np.zeros((X.shape[0],))   # training set probabilities\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(skfold.split(X, y)):\n",
    "        # Select train, validation set by each fold\n",
    "        X_train, y_train = X.iloc[train_idx, :], y[train_idx]\n",
    "        X_val, y_val = X.iloc[val_idx, :], y[val_idx]\n",
    "\n",
    "        # Train Model\n",
    "        model = clone(model)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Evaluate by validation set\n",
    "        val_probs = model.predict_proba(X_val)[:, 1]   # Calculate probability\n",
    "        oof_probs[val_idx] = val_probs   # Save probability of this fold's validation set\n",
    "        val_preds = (val_probs > 0.5).astype(int)   # Convert probability to label\n",
    "        mcc = matthews_corrcoef(y_val, val_preds)   # Calculate mcc score\n",
    "        oof_mccs.append(mcc)   # Save mcc score of this fold's validation set\n",
    "        print(f\"--- Fold {fold+1} MCC Score: {mcc:.6f}\")\n",
    "\n",
    "        # Predict on test set\n",
    "        test_probs += model.predict_proba(test_set)[:, 1] / skfold.get_n_splits()   # Aggregate test set probabilities\n",
    "\n",
    "    mean_mcc_score = np.mean(oof_mccs)\n",
    "    std_mcc_score = np.std(oof_mccs)\n",
    "    print(f\"\\n---> Mean MCC Score: {mean_mcc_score:.6f} \\xb1 {std_mcc_score:.6f}\\n\\n\")\n",
    "    \n",
    "    total_mcc_score = matthews_corrcoef(y, (oof_probs > 0.5).astype(int))\n",
    "    print(f\"Total MCC Score: {total_mcc_score:.6f}\")\n",
    "\n",
    "    return oof_mccs, test_probs, oof_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' tree-based models '''\n",
    "\n",
    "xgb_clf = XGBClassifier(enable_categorical=True, device=\"cuda\", tree_method=\"hist\")\n",
    "cat_clf = CatBoostClassifier(\n",
    "    cat_features=categorical_features,\n",
    "    verbose=False,\n",
    "    allow_writing_files=False,\n",
    "    task_type=\"GPU\"\n",
    ")\n",
    "lgb_clf = LGBMClassifier(device='gpu', verbosity=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724915040.929438    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.040894    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.043381    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.048605    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.050631    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.052508    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.133255    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.134194    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "I0000 00:00:1724915041.134978    5766 cuda_executor.cc:1015] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-29 16:04:01.136283: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9633 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4070 Ti, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) for `plot_model` to work.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Neural Network models '''\n",
    "\n",
    "# Neural network (Keras) with two hidden layers (one-hot encoded)\n",
    "def get_model(meta):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Input(meta[\"X_shape_\"][1:]))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "\n",
    "display(keras.utils.plot_model(get_model({\"X_shape_\": X.shape}),\n",
    "                       show_shapes=True, show_layer_activations=True, dpi=80))\n",
    "\n",
    "\n",
    "nn_clf = KerasClassifier(\n",
    "    get_model,\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=keras.optimizers.AdamW(learning_rate=0.01),\n",
    "    validation_split=0.03,\n",
    "    batch_size=8192,\n",
    "    validation_batch_size=65536,\n",
    "    epochs=53,\n",
    "    # verbose=0, # or 2\n",
    "    callbacks=[keras.callbacks.ReduceLROnPlateau(patience=3),\n",
    "               keras.callbacks.EarlyStopping(patience=5, min_delta=0.00003)]\n",
    ")\n",
    "\n",
    "nn_model = make_pipeline(ColumnTransformer([\n",
    "                                        ('float', make_pipeline(PowerTransformer(),\n",
    "                                                                 SimpleImputer(add_indicator=True)),\n",
    "                                          numerical_features),\n",
    "                                         ('cat', OneHotEncoder(drop='first',\n",
    "                                                               sparse_output=False,\n",
    "                                                               dtype=np.float32,\n",
    "                                                               handle_unknown='infrequent_if_exist',\n",
    "                                                               min_frequency=50),\n",
    "                                          categorical_features),\n",
    "                                        ]),\n",
    "                      StandardScaler(),\n",
    "                      BaggingClassifier(nn_clf, n_estimators=7, bootstrap=False))\n",
    "# cross_validate(nn_model, 'Keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = {\n",
    "    'n_estimators': 2407,\n",
    "    'eta': 0.009462133032592785,\n",
    "    'gamma': 0.2865859948765318,\n",
    "    'max_depth': 31,\n",
    "    'min_child_weight': 47,\n",
    "    'subsample': 0.6956431754146083,\n",
    "    'colsample_bytree': 0.3670732604094118,\n",
    "    'grow_policy': 'lossguide',\n",
    "    'max_leaves': 73,\n",
    "    'enable_categorical': True,\n",
    "    'n_jobs': -1,\n",
    "    'device': 'cuda',\n",
    "    'tree_method': 'hist'\n",
    "} # 0.9844272567086021\n",
    "\n",
    "cat_params = {\n",
    "    'iterations': 1041,\n",
    "    'learning_rate': 0.08777255350163136,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 0.1259643500248322,\n",
    "    'bootstrap_type': 'Bayesian',\n",
    "    'random_strength': 4.276181166674371e-08,\n",
    "    'bagging_temperature': 0.35995482350907326,\n",
    "    'od_type': 'Iter',\n",
    "    'od_wait': 39,\n",
    "    \"verbose\": False,\n",
    "    \"allow_writing_files\": False,\n",
    "    \"task_type\": 'GPU',\n",
    "    \"cat_features\": categorical_features\n",
    "} # 0.9841773055825763\n",
    "\n",
    "lgb_params = {\n",
    "    'n_estimators': 2500,\n",
    "    'random_state':42,\n",
    "    'max_bin':1024,\n",
    "    'colsample_bytree':0.6,\n",
    "    'reg_lambda': 80,\n",
    "    # 'device': 'gpu',\n",
    "    'verbosity': -1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% time\n",
    "# cv_summary, test_probs, oof_probs = pd.DataFrame(), pd.DataFrame(), pd.DataFrame()\n",
    "cv_summary, test_probs, oof_probs = pd.read_pickle(\"result/deep_nn_test1/cv_summary.pkl\"), pd.read_pickle(\"result/deep_nn_test1/test_probs.pkl\"), pd.read_pickle(\"result/deep_nn_test1/oof_probs.pkl\")\n",
    "random_state = 101\n",
    "skfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_summary['xgb'], test_probs['xgb'], oof_probs['xgb'] = model_trainer(XGBClassifier(**xgb_params), X, y, test_df, skfold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_summary['cat'], test_probs['cat'], oof_probs['cat'] = model_trainer(CatBoostClassifier(**cat_params), X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# cv_summary['lgb'], test_probs['lgb'], oof_probs['lgb'] = model_trainer(LGBMClassifier(**lgb_params), X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Training Pipeline\n",
      "================================================================================\n",
      "Epoch 1/53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724915652.010767   15696 service.cc:146] XLA service 0x73fa50003b50 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724915652.010784   15696 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 4070 Ti, Compute Capability 8.9\n",
      "2024-08-29 16:14:12.035224: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-29 16:14:12.168874: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8905\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 66/302\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.2850"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1724915652.642566   15696 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1268 - val_loss: 0.0217 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0418 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0385 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0380 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0369 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0361 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0356 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1367 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0413 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0402 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0168 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0378 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0367 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0359 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1184 - val_loss: 0.0216 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0414 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0400 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0387 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0370 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0362 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0359 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0171 - learning_rate: 1.0000e-04\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0353 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0352 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0171 - learning_rate: 1.0000e-05\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0351 - val_loss: 0.0171 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1378 - val_loss: 0.0212 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0419 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0406 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0385 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0380 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0370 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0355 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0354 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0350 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0349 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1256 - val_loss: 0.0240 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0418 - val_loss: 0.0211 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0405 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0387 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0371 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0361 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0359 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1198 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0418 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0404 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0386 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0166 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0381 - val_loss: 0.0167 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0165 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0378 - val_loss: 0.0163 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 0.0376 - val_loss: 0.0167 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0375 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1401 - val_loss: 0.0248 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0421 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0406 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0396 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0391 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0381 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0370 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0354 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0353 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0352 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0352 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0352 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0169 - learning_rate: 1.0000e-06\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "--- Fold 1 MCC Score: 0.984434\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1365 - val_loss: 0.0231 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0409 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0399 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0396 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0384 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0380 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0379 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0378 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0381 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0376 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0367 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0361 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1384 - val_loss: 0.0264 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0426 - val_loss: 0.0207 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0409 - val_loss: 0.0214 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0402 - val_loss: 0.0217 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0208 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0216 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0214 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0183 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0184 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0362 - val_loss: 0.0184 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0361 - val_loss: 0.0183 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0358 - val_loss: 0.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0357 - val_loss: 0.0178 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0356 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0355 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 0.0355 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - val_loss: 0.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1117 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0422 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0400 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0396 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0389 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0388 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0364 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1440 - val_loss: 0.0302 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0422 - val_loss: 0.0218 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0399 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0391 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0382 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0381 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0380 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0378 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0377 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0376 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0375 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0373 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0373 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0372 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0371 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0372 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0370 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 27/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 28/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 29/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0346 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1367 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0425 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0412 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0395 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0369 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0366 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0364 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0362 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0361 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1100 - val_loss: 0.0251 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0427 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0413 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0405 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0395 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0390 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0389 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0382 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0378 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0366 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0354 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0351 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0174 - learning_rate: 1.0000e-04\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0349 - val_loss: 0.0175 - learning_rate: 1.0000e-05\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0175 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1167 - val_loss: 0.0233 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0425 - val_loss: 0.0219 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0410 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0219 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0225 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0395 - val_loss: 0.0216 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0369 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0366 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0173 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0360 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0360 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0359 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0359 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0359 - val_loss: 0.0170 - learning_rate: 1.0000e-05\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "--- Fold 2 MCC Score: 0.984437\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1190 - val_loss: 0.0221 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0410 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0398 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0376 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0375 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0373 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0371 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0370 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0348 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0346 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0345 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0344 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0343 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0341 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1317 - val_loss: 0.0263 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0425 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0411 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0405 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0401 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0396 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0370 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0363 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1152 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0416 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0389 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0385 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0370 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0360 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0164 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0352 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0166 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0166 - learning_rate: 1.0000e-05\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0166 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1408 - val_loss: 0.0212 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0418 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0408 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0399 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0397 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0385 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0365 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1280 - val_loss: 0.0225 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0424 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0404 - val_loss: 0.0190 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0397 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0393 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0382 - val_loss: 0.0171 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0382 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0367 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0359 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1140 - val_loss: 0.0280 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0417 - val_loss: 0.0226 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0228 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0214 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0393 - val_loss: 0.0211 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0391 - val_loss: 0.0211 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0388 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0382 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0379 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0203 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0377 - val_loss: 0.0203 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0376 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0364 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0349 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0348 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0173 - learning_rate: 1.0000e-04\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0346 - val_loss: 0.0174 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1423 - val_loss: 0.0203 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0416 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0403 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0392 - val_loss: 0.0169 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0389 - val_loss: 0.0165 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0164 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0385 - val_loss: 0.0167 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0167 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0165 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0370 - val_loss: 0.0166 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0167 - learning_rate: 1.0000e-03\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "--- Fold 3 MCC Score: 0.984248\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1328 - val_loss: 0.0228 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0424 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0372 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0363 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0361 - val_loss: 0.0169 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0359 - val_loss: 0.0170 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1363 - val_loss: 0.0319 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0430 - val_loss: 0.0272 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0415 - val_loss: 0.0256 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0407 - val_loss: 0.0245 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0401 - val_loss: 0.0230 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0397 - val_loss: 0.0225 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0224 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0391 - val_loss: 0.0220 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0389 - val_loss: 0.0213 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0387 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0384 - val_loss: 0.0203 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0381 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0375 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0375 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0374 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0373 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0365 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0355 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0351 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 27/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0349 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 28/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0348 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 29/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 30/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0347 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 31/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0346 - val_loss: 0.0171 - learning_rate: 1.0000e-05\n",
      "Epoch 32/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0346 - val_loss: 0.0171 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1449 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0417 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0403 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0397 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0379 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0369 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0366 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0364 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0360 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0359 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0359 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0358 - val_loss: 0.0173 - learning_rate: 1.0000e-05\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0358 - val_loss: 0.0173 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1089 - val_loss: 0.0239 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0423 - val_loss: 0.0216 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0413 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0406 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0400 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0397 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0396 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0388 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0387 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0371 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0359 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0353 - val_loss: 0.0173 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1186 - val_loss: 0.0247 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0426 - val_loss: 0.0207 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0408 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0403 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0397 - val_loss: 0.0197 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0393 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0390 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0385 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0370 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0362 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0359 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0358 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0175 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0175 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.1537 - val_loss: 0.0239 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0423 - val_loss: 0.0222 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0408 - val_loss: 0.0215 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0214 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0395 - val_loss: 0.0212 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0391 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0389 - val_loss: 0.0203 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0386 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0385 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0366 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0356 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0354 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0350 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0350 - val_loss: 0.0172 - learning_rate: 1.0000e-04\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0172 - learning_rate: 1.0000e-05\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0172 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1157 - val_loss: 0.0242 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0421 - val_loss: 0.0226 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0407 - val_loss: 0.0241 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0401 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0396 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0199 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0389 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0385 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0367 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0168 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0170 - learning_rate: 1.0000e-04\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "--- Fold 4 MCC Score: 0.984384\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1307 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0416 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0403 - val_loss: 0.0207 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0202 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0388 - val_loss: 0.0196 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0195 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0371 - val_loss: 0.0177 - learning_rate: 1.0000e-03\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0176 - learning_rate: 1.0000e-03\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0359 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0357 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0351 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0349 - val_loss: 0.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - loss: 0.1044 - val_loss: 0.0219 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0412 - val_loss: 0.0207 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0402 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0395 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0384 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0381 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0380 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0379 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0367 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0354 - val_loss: 0.0171 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0171 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1191 - val_loss: 0.0245 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0419 - val_loss: 0.0221 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0408 - val_loss: 0.0194 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0399 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0394 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0391 - val_loss: 0.0193 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0389 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0386 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0385 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0381 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0379 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0378 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0376 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0181 - learning_rate: 1.0000e-03\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0352 - val_loss: 0.0179 - learning_rate: 1.0000e-04\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1045 - val_loss: 0.0218 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0414 - val_loss: 0.0201 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0403 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0396 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0388 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0176 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0383 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0174 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0379 - val_loss: 0.0170 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0173 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0172 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1278 - val_loss: 0.0265 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0425 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0198 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0398 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0392 - val_loss: 0.0188 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0391 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0389 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0384 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0383 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0371 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0362 - val_loss: 0.0181 - learning_rate: 1.0000e-03\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0359 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0358 - val_loss: 0.0182 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0354 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0354 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0177 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0353 - val_loss: 0.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0177 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - loss: 0.1276 - val_loss: 0.0252 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0424 - val_loss: 0.0200 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0407 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0399 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0395 - val_loss: 0.0181 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0186 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0384 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0382 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0386 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0375 - val_loss: 0.0183 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0365 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0357 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0355 - val_loss: 0.0174 - learning_rate: 1.0000e-03\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0353 - val_loss: 0.0175 - learning_rate: 1.0000e-03\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0351 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0349 - val_loss: 0.0176 - learning_rate: 1.0000e-04\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0348 - val_loss: 0.0176 - learning_rate: 1.0000e-05\n",
      "Epoch 1/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - loss: 0.1136 - val_loss: 0.0217 - learning_rate: 0.0100\n",
      "Epoch 2/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0417 - val_loss: 0.0215 - learning_rate: 0.0100\n",
      "Epoch 3/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0405 - val_loss: 0.0210 - learning_rate: 0.0100\n",
      "Epoch 4/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0399 - val_loss: 0.0206 - learning_rate: 0.0100\n",
      "Epoch 5/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0394 - val_loss: 0.0209 - learning_rate: 0.0100\n",
      "Epoch 6/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0390 - val_loss: 0.0205 - learning_rate: 0.0100\n",
      "Epoch 7/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0387 - val_loss: 0.0204 - learning_rate: 0.0100\n",
      "Epoch 8/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0385 - val_loss: 0.0191 - learning_rate: 0.0100\n",
      "Epoch 9/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0382 - val_loss: 0.0192 - learning_rate: 0.0100\n",
      "Epoch 10/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0382 - val_loss: 0.0189 - learning_rate: 0.0100\n",
      "Epoch 11/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0380 - val_loss: 0.0184 - learning_rate: 0.0100\n",
      "Epoch 12/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0187 - learning_rate: 0.0100\n",
      "Epoch 13/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0377 - val_loss: 0.0185 - learning_rate: 0.0100\n",
      "Epoch 14/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0375 - val_loss: 0.0180 - learning_rate: 0.0100\n",
      "Epoch 15/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0374 - val_loss: 0.0179 - learning_rate: 0.0100\n",
      "Epoch 16/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0373 - val_loss: 0.0182 - learning_rate: 0.0100\n",
      "Epoch 17/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0375 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 18/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0373 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 19/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0372 - val_loss: 0.0175 - learning_rate: 0.0100\n",
      "Epoch 20/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0371 - val_loss: 0.0178 - learning_rate: 0.0100\n",
      "Epoch 21/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0369 - val_loss: 0.0177 - learning_rate: 0.0100\n",
      "Epoch 22/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0360 - val_loss: 0.0172 - learning_rate: 1.0000e-03\n",
      "Epoch 23/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0352 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 24/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0350 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 25/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0348 - val_loss: 0.0171 - learning_rate: 1.0000e-03\n",
      "Epoch 26/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0346 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 27/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0345 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 28/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0345 - val_loss: 0.0168 - learning_rate: 1.0000e-04\n",
      "Epoch 29/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0344 - val_loss: 0.0169 - learning_rate: 1.0000e-04\n",
      "Epoch 30/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0344 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "Epoch 31/53\n",
      "\u001b[1m302/302\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0344 - val_loss: 0.0169 - learning_rate: 1.0000e-05\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "--- Fold 5 MCC Score: 0.984603\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\u001b[1m254/254\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "\n",
      "---> Mean MCC Score: 0.984421 ± 0.000114\n",
      "\n",
      "\n",
      "Total MCC Score: 0.984421\n"
     ]
    }
   ],
   "source": [
    "cv_summary['nn'], test_probs['nn'], oof_probs['nn'] = model_trainer(nn_model, X, y, test_df, skfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_b8d91_row0_col0, #T_b8d91_row0_col1, #T_b8d91_row0_col2, #T_b8d91_row0_col3, #T_b8d91_row0_col4, #T_b8d91_row0_col5, #T_b8d91_row1_col1, #T_b8d91_row1_col3, #T_b8d91_row2_col6 {\n",
       "  background-color: #1b9e77;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b8d91_row0_col6, #T_b8d91_row3_col0, #T_b8d91_row3_col1, #T_b8d91_row3_col2, #T_b8d91_row3_col3, #T_b8d91_row3_col4, #T_b8d91_row3_col5 {\n",
       "  background-color: #666666;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b8d91_row1_col0, #T_b8d91_row1_col2, #T_b8d91_row1_col4, #T_b8d91_row1_col5, #T_b8d91_row2_col1, #T_b8d91_row2_col3 {\n",
       "  background-color: #d95f02;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b8d91_row1_col6, #T_b8d91_row2_col4 {\n",
       "  background-color: #a6761d;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b8d91_row2_col0, #T_b8d91_row2_col5 {\n",
       "  background-color: #e7298a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_b8d91_row2_col2 {\n",
       "  background-color: #e6ab02;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_b8d91_row3_col6 {\n",
       "  background-color: #7570b3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_b8d91\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_b8d91_level0_col0\" class=\"col_heading level0 col0\" >fold1</th>\n",
       "      <th id=\"T_b8d91_level0_col1\" class=\"col_heading level0 col1\" >fold2</th>\n",
       "      <th id=\"T_b8d91_level0_col2\" class=\"col_heading level0 col2\" >fold3</th>\n",
       "      <th id=\"T_b8d91_level0_col3\" class=\"col_heading level0 col3\" >fold4</th>\n",
       "      <th id=\"T_b8d91_level0_col4\" class=\"col_heading level0 col4\" >fold5</th>\n",
       "      <th id=\"T_b8d91_level0_col5\" class=\"col_heading level0 col5\" >Mean</th>\n",
       "      <th id=\"T_b8d91_level0_col6\" class=\"col_heading level0 col6\" >Std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_b8d91_level0_row0\" class=\"row_heading level0 row0\" >lgb</th>\n",
       "      <td id=\"T_b8d91_row0_col0\" class=\"data row0 col0\" >0.985025</td>\n",
       "      <td id=\"T_b8d91_row0_col1\" class=\"data row0 col1\" >0.984989</td>\n",
       "      <td id=\"T_b8d91_row0_col2\" class=\"data row0 col2\" >0.984879</td>\n",
       "      <td id=\"T_b8d91_row0_col3\" class=\"data row0 col3\" >0.984977</td>\n",
       "      <td id=\"T_b8d91_row0_col4\" class=\"data row0 col4\" >0.984971</td>\n",
       "      <td id=\"T_b8d91_row0_col5\" class=\"data row0 col5\" >0.984968</td>\n",
       "      <td id=\"T_b8d91_row0_col6\" class=\"data row0 col6\" >0.000048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b8d91_level0_row1\" class=\"row_heading level0 row1\" >xgb</th>\n",
       "      <td id=\"T_b8d91_row1_col0\" class=\"data row1 col0\" >0.984879</td>\n",
       "      <td id=\"T_b8d91_row1_col1\" class=\"data row1 col1\" >0.984927</td>\n",
       "      <td id=\"T_b8d91_row1_col2\" class=\"data row1 col2\" >0.984749</td>\n",
       "      <td id=\"T_b8d91_row1_col3\" class=\"data row1 col3\" >0.984930</td>\n",
       "      <td id=\"T_b8d91_row1_col4\" class=\"data row1 col4\" >0.984882</td>\n",
       "      <td id=\"T_b8d91_row1_col5\" class=\"data row1 col5\" >0.984873</td>\n",
       "      <td id=\"T_b8d91_row1_col6\" class=\"data row1 col6\" >0.000066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b8d91_level0_row2\" class=\"row_heading level0 row2\" >cat</th>\n",
       "      <td id=\"T_b8d91_row2_col0\" class=\"data row2 col0\" >0.984749</td>\n",
       "      <td id=\"T_b8d91_row2_col1\" class=\"data row2 col1\" >0.984860</td>\n",
       "      <td id=\"T_b8d91_row2_col2\" class=\"data row2 col2\" >0.984470</td>\n",
       "      <td id=\"T_b8d91_row2_col3\" class=\"data row2 col3\" >0.984838</td>\n",
       "      <td id=\"T_b8d91_row2_col4\" class=\"data row2 col4\" >0.984675</td>\n",
       "      <td id=\"T_b8d91_row2_col5\" class=\"data row2 col5\" >0.984718</td>\n",
       "      <td id=\"T_b8d91_row2_col6\" class=\"data row2 col6\" >0.000140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_b8d91_level0_row3\" class=\"row_heading level0 row3\" >nn</th>\n",
       "      <td id=\"T_b8d91_row3_col0\" class=\"data row3 col0\" >0.984434</td>\n",
       "      <td id=\"T_b8d91_row3_col1\" class=\"data row3 col1\" >0.984437</td>\n",
       "      <td id=\"T_b8d91_row3_col2\" class=\"data row3 col2\" >0.984248</td>\n",
       "      <td id=\"T_b8d91_row3_col3\" class=\"data row3 col3\" >0.984384</td>\n",
       "      <td id=\"T_b8d91_row3_col4\" class=\"data row3 col4\" >0.984603</td>\n",
       "      <td id=\"T_b8d91_row3_col5\" class=\"data row3 col5\" >0.984421</td>\n",
       "      <td id=\"T_b8d91_row3_col6\" class=\"data row3 col6\" >0.000114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x73fb18017df0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performance Summary for comparing model (Not Needed)\n",
    "transposed_df = cv_summary.transpose()\n",
    "transposed_df.columns = ['fold1','fold2','fold3','fold4','fold5']\n",
    "transposed_df['Mean'] = transposed_df.mean(axis=1)\n",
    "transposed_df['Std'] = transposed_df.std(axis=1)\n",
    "transposed_df.sort_values(by = 'Mean', ascending=False).style.background_gradient('Dark2_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x2000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model diversity check (Not Needed)\n",
    "sns.set(font_scale=1.1)\n",
    "correlation_train = oof_probs.corr()\n",
    "mask = np.triu(correlation_train.corr())\n",
    "plt.figure(figsize=(20, 20))\n",
    "sns.heatmap(correlation_train,\n",
    "            annot=True,\n",
    "            fmt='.3f',\n",
    "            cmap='coolwarm',\n",
    "            square=True,\n",
    "            mask=mask,\n",
    "            linewidths=1,\n",
    "            cbar=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting hard & soft\n",
    "def voting_ensemble(oof_probs, y, threshold=0.5, voting_type='soft'):\n",
    "    if voting_type == 'soft':\n",
    "        ensemble_preds = oof_probs.mean(axis=1)\n",
    "        ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n",
    "        \n",
    "    elif voting_type == 'hard':\n",
    "        binary_preds = (oof_probs > threshold).astype(int)\n",
    "        ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n",
    "    \n",
    "    mcc_score = matthews_corrcoef(y, ensemble_class_preds)\n",
    "    \n",
    "    return mcc_score, ensemble_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.985191428825491\n"
     ]
    }
   ],
   "source": [
    "soft_vote_score, soft_vote_pred = voting_ensemble(oof_probs, y, voting_type='soft')\n",
    "print(soft_vote_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9850697456089739\n"
     ]
    }
   ],
   "source": [
    "hard_vote_score, hard_vote_pred = voting_ensemble(oof_probs, y, voting_type='hard')\n",
    "print(hard_vote_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting hard & soft\n",
    "def voting_ensemble_only_pred(oof_probs, threshold=0.5, voting_type='soft'):\n",
    "  if voting_type == 'soft':\n",
    "      ensemble_preds = oof_probs.mean(axis=1)\n",
    "      ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n",
    "      \n",
    "  elif voting_type == 'hard':\n",
    "      binary_preds = (oof_probs > threshold).astype(int)\n",
    "      ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n",
    "\n",
    "  return ensemble_class_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "soft_vote_pred_test = voting_ensemble_only_pred(test_probs, voting_type='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0          0\n",
      "1          1\n",
      "2          1\n",
      "3          1\n",
      "4          0\n",
      "          ..\n",
      "2077959    1\n",
      "2077960    1\n",
      "2077961    1\n",
      "2077962    0\n",
      "2077963    0\n",
      "Length: 2077964, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(soft_vote_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters for meta model                                                                                                 \n",
    "meta_model_params = {\n",
    "    'C': 0.000237302749626327,\n",
    "    'max_iter': 2500,\n",
    "    'tol': 9.996751434702547e-05,\n",
    "    'solver': 'saga',\n",
    "    'penalty': 'l1'\n",
    "}\n",
    "\n",
    "meta_model = LogisticRegression(**meta_model_params, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: \n",
      "0.9851492829019672\n",
      "Number of available models: 4\n",
      "Number of selected models for ensemble: 4\n",
      "Selected models: ['xgb' 'cat' 'lgb' 'nn']\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "\n",
    "#Deciding which models to include ensemble\n",
    "\n",
    "min_features_to_select = 1\n",
    "\n",
    "# Create a pipeline with preprocessor and RFECV\n",
    "pipeline = Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('rfecv', RFECV(estimator=meta_model,\n",
    "                    step=1,\n",
    "                    cv=skfold,\n",
    "                    scoring=make_scorer(matthews_corrcoef),\n",
    "                    min_features_to_select=min_features_to_select,\n",
    "                    n_jobs=-1,))\n",
    "])\n",
    "\n",
    "# Fit the pipeline on oof_preds\n",
    "pipeline.fit(oof_probs, y)\n",
    "\n",
    "#CV score\n",
    "print(\"Best CV score: \")\n",
    "selected_models = np.array(oof_probs.columns)[pipeline.named_steps['rfecv'].support_]\n",
    "print( pipeline.named_steps['rfecv'].cv_results_[\"mean_test_score\"][len(selected_models) - 1])\n",
    "\n",
    "\n",
    "# Selected models after RFECV\n",
    "print('Number of available models:', len(oof_probs.columns))\n",
    "print('Number of selected models for ensemble:', len(selected_models))\n",
    "print(\"Selected models:\", selected_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_model = meta_model.fit(oof_probs[selected_models], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9851582093616669\n"
     ]
    }
   ],
   "source": [
    "preds_train =  meta_model.predict(oof_probs[selected_models])\n",
    "mcc_score = matthews_corrcoef(y, preds_train)\n",
    "print(mcc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_test =  meta_model.predict(test_probs[selected_models])\n",
    "preds_test = lab_enc.inverse_transform(preds_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Intermediate Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_summary['lgb'], test_probs['lgb'], oof_probs['lgb']\n",
    "exp_name = \"4_layers_nn\"\n",
    "\n",
    "from pathlib import Path\n",
    "Path(f\"result/{exp_name}\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "cv_summary.to_pickle(f\"result/{exp_name}/cv_summary.pkl\")\n",
    "test_probs.to_pickle(f\"result/{exp_name}/test_probs.pkl\")\n",
    "oof_probs.to_pickle(f\"result/{exp_name}/oof_probs.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Stacking '''\n",
    "\n",
    "output = pd.DataFrame({'id': test_df.index,\n",
    "                       'class': preds_test})\n",
    "output.to_csv('pred/submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ''' Soft Voting '''\n",
    "# soft_vote_pred_test = lab_enc.inverse_transform(soft_vote_pred_test)\n",
    "# output = pd.DataFrame({'id': test_df.index,\n",
    "#                        'class': soft_vote_pred_test})\n",
    "# output.to_csv('pred/submission_soft_vote_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #save oofs and test predictions for later usage\n",
    "# oof_probs.to_parquet('oof_predictions_v01.parquet', index=False)\n",
    "# test_probs.to_parquet('test_predictions_v01.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sub = pd.read_csv(\"/data/playground-series-s4e8/sample_submission.csv\")\n",
    "# preds = [pred for model, pred in oof_preds.items()]\n",
    "# md = mode(preds, axis=0)[0] if len(preds)>1 else preds[0]\n",
    "# sub[target] = lab_enc.inverse_transform(md)\n",
    "# sub.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ext1 = pd.read_csv(\"/kaggle/input/mario-s-nightmare-15-th-place-solution/submission.csv\")[target].ravel()\n",
    "# ext2 = pd.read_csv(\"/kaggle/input/ps4e8-binary-class-mathews-correlation-coeff/submission.csv\")[target].ravel()\n",
    "# ext3 = pd.read_csv(\"/kaggle/input/playgrounds4e08-modeblend/submission.csv\")[target].ravel()\n",
    "# ext4 = pd.read_csv(\"/kaggle/input/autogloun-t8-dslanders/submission.csv\")[target].ravel()\n",
    "# ext5 = pd.read_csv(\"/kaggle/input/mario-s-nightmare-denselight-0-990/submission_test7.csv\")[target].ravel()\n",
    "\n",
    "# preds = [ext1, ext2, ext3, ext4, ext5]\n",
    "# preds = [lab_enc.transform(x) for x in preds]\n",
    "# md = mode(preds, axis=0)[0]\n",
    "# sub[target] = lab_enc.inverse_transform(md)\n",
    "# sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
