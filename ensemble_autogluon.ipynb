{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from scipy.stats import mode\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load baseline ensemble version & Autogluon result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load probabilities from CSV files\n",
    "# Result from \"only competition dataset\"\n",
    "# (Try to use original dataset too if it has better performance)\n",
    "# Auto 의 경우 prob 계산을 우리 형태에 맞게 다시 해줘야 될듯\n",
    "auto_train_df = pd.read_csv('pred/submission_autogluon_train_proba.csv')\n",
    "base_train_df = pd.read_csv('pred/submission_stack_base_train_proba.csv')\n",
    "auto_train_probs = auto_train_df['class']\n",
    "base_train_probs = base_train_df['class']\n",
    "\n",
    "auto_test_df = pd.read_csv('pred/submission_autogluon_test_proba.csv')\n",
    "base_test_df = pd.read_csv('pred/submission_stack_base_test_proba.csv')\n",
    "auto_test_probs = auto_test_df['class']\n",
    "base_test_probs = base_test_df['class']\n",
    "\n",
    "train_df = pd.read_csv(\"data/playground-series-s4e8/train.csv\", index_col='id')\n",
    "train_y = train_df['class']\n",
    "lab_enc = LabelEncoder().fit(train_y)\n",
    "train_y = lab_enc.transform(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auto_train_probs.shape)\n",
    "print(base_train_probs.shape)\n",
    "\n",
    "print(auto_test_probs.shape)\n",
    "print(base_test_probs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Ensemble Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#voting hard & soft\n",
    "#voting hard & soft\n",
    "def voting_ensemble(oof_probs, y, threshold=0.5, voting_type='soft'):\n",
    "    if voting_type == 'soft':\n",
    "        ensemble_preds = oof_probs.mean(axis=1)\n",
    "        ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n",
    "        \n",
    "    elif voting_type == 'hard':\n",
    "        binary_preds = (oof_probs > threshold).astype(int)\n",
    "        ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n",
    "    \n",
    "    mcc_score = matthews_corrcoef(y, ensemble_class_preds)\n",
    "    \n",
    "    return mcc_score, ensemble_class_preds\n",
    "\n",
    "def voting_ensemble_only_pred(oof_probs, threshold=0.5, voting_type='soft'):\n",
    "  if voting_type == 'soft':\n",
    "      ensemble_preds = oof_probs.mean(axis=1)\n",
    "      ensemble_class_preds = (ensemble_preds > threshold).astype(int)\n",
    "      \n",
    "  elif voting_type == 'hard':\n",
    "      binary_preds = (oof_probs > threshold).astype(int)\n",
    "      ensemble_class_preds = mode(binary_preds, axis=1)[0].flatten()\n",
    "\n",
    "  return ensemble_class_preds\n",
    "train_df = pd.read_csv(\"data/playground-series-s4e8/train.csv\", index_col='id')\n",
    "\n",
    "random_state = 101\n",
    "meta_model = LogisticRegression(**metatest_probs_model_params, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conduct Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_probs, test_probs = pd.DataFrame(), pd.DataFrame()\n",
    "\n",
    "train_probs['autogluon'] = auto_train_probs\n",
    "train_probs['base_ensemble'] = base_train_probs\n",
    "\n",
    "test_probs['autogluon'] = auto_test_probs\n",
    "test_probs['base_ensemble'] = base_test_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CV score\n",
    "train_soft_vote_score, train_soft_vote_pred = voting_ensemble(train_probs, train_y, voting_type='soft')\n",
    "train_hard_vote_score, train_hard_vote_pred = voting_ensemble(train_probs, train_y, voting_type='hard')\n",
    "\n",
    "print(f\"Soft Voting MCC (CV score): {train_soft_vote_score}\")\n",
    "print(f\"Hard Voting MCC (CV score): {train_hard_vote_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Test predictions\n",
    "test_soft_vote_pred = voting_ensemble_only_pred(test_probs, voting_type='soft')\n",
    "test_hard_vote_pred = voting_ensemble_only_pred(test_probs, voting_type='hard')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Meta Model\n",
    "meta_model = meta_model.fit(train_probs, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get CV score\n",
    "train_stack_pred = meta_model.predict(train_probs)\n",
    "train_stack_score = matthews_corrcoef(train_y, train_stack_pred)\n",
    "\n",
    "print(f\"Tacking MCC (CV score): {train_stack_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Test predictions\n",
    "test_stack_pred = meta_model.predict(test_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Soft Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Soft Voting '''\n",
    "test_soft_vote_pred = lab_enc.inverse_transform(test_soft_vote_pred)\n",
    "sub = pd.read_csv('data/playground-series-s4e8/sample_submission.csv')\n",
    "sub['class'] = test_soft_vote_pred\n",
    "sub.to_csv('pred/sub_auto_base_soft_vote.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Hard Vote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Hard Voting '''\n",
    "test_hard_vote_pred = lab_enc.inverse_transform(test_hard_vote_pred)\n",
    "sub = pd.read_csv('data/playground-series-s4e8/sample_submission.csv')\n",
    "sub['class'] = test_hard_vote_pred\n",
    "sub.to_csv('pred/sub_auto_base_hard_vote.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Stacking '''\n",
    "test_stack_pred = lab_enc.inverse_transform(test_stack_pred)\n",
    "sub = pd.read_csv('data/playground-series-s4e8/sample_submission.csv')\n",
    "sub['class'] = test_stack_pred\n",
    "sub.to_csv('pred/sub_auto_base_stack.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
