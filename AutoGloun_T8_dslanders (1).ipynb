{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 76727,
          "databundleVersionId": 9045607,
          "sourceType": "competition"
        },
        {
          "sourceId": 192808909,
          "sourceType": "kernelVersion"
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "AutoGloun-T8-dslanders",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'playground-series-s4e8:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-competitions-data%2Fkaggle-v2%2F76727%2F9045607%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240829%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240829T123310Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D7880b796162fdda0745a833b42b74acc16bdf57f7d25cd1313b636d71b450c476b9f2f1ce4b577ffa0e142c33d5a49daa2747ffb05bd258a524b139da74dfba3656bfa5ef26259e4833616e893e35178d4ce76bd147b7842be18af8beb6f9386caa15a2a63d686d12ca15d9f40743fc626d95387cd8d89053665246e71e498110b2a35a1a4fca772107a52bba2e9a7dfd301054d1e822ae0484f4747643c02127b0c0175bb24cf14dcf93a047db161da0b9019597dfb87082dfe3623024029dd4c920a8f44d8ff70ffb335b795c264fde9d431490ba046bf6b9fc4d4457631975617b0f658225fd2657b9ccce6ea5b05ecfe8e1a8b0bf8a4a887c4ed78a3f9cc'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8uG4BC1IC6z",
        "outputId": "e7a00b44-37fc-491c-e543-18cca245be12"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading playground-series-s4e8, 86301661 bytes compressed\n",
            "[==================================================] 86301661 bytes downloaded\n",
            "Downloaded and uncompressed: playground-series-s4e8\n",
            "Data source import complete.\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ray==2.10.0\n",
        "!pip install autogluon.tabular\n",
        "!pip install -U ipywidgets"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:46:27.116299Z",
          "iopub.execute_input": "2024-08-13T13:46:27.116832Z",
          "iopub.status.idle": "2024-08-13T13:47:43.809341Z",
          "shell.execute_reply.started": "2024-08-13T13:46:27.116795Z",
          "shell.execute_reply": "2024-08-13T13:47:43.808006Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytYwvXeRIC60",
        "outputId": "5d3e511e-4ef2-4b70-9769-9e0377c0b0d7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ray==2.10.0\n",
            "  Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (8.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (3.15.4)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (4.23.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (1.0.8)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (24.1)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (3.20.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (6.0.2)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (1.3.1)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from ray==2.10.0) (2.32.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->ray==2.10.0) (0.20.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->ray==2.10.0) (2024.7.4)\n",
            "Downloading ray-2.10.0-cp310-cp310-manylinux2014_x86_64.whl (65.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.1/65.1 MB\u001b[0m \u001b[31m34.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ray\n",
            "Successfully installed ray-2.10.0\n",
            "Collecting autogluon.tabular\n",
            "  Downloading autogluon.tabular-1.1.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (1.26.4)\n",
            "Collecting scipy<1.13,>=1.5.4 (from autogluon.tabular)\n",
            "  Downloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (2.1.4)\n",
            "Requirement already satisfied: scikit-learn<1.4.1,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (1.3.2)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (3.3)\n",
            "Collecting autogluon.core==1.1.1 (from autogluon.tabular)\n",
            "  Downloading autogluon.core-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.features==1.1.1 (from autogluon.tabular)\n",
            "  Downloading autogluon.features-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular) (4.66.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular) (2.32.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.1->autogluon.tabular) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.1.1->autogluon.tabular)\n",
            "  Downloading boto3-1.35.8-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting autogluon.common==1.1.1 (from autogluon.core==1.1.1->autogluon.tabular)\n",
            "  Downloading autogluon.common-1.1.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.1->autogluon.core==1.1.1->autogluon.tabular) (71.0.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (3.5.0)\n",
            "Collecting botocore<1.36.0,>=1.35.8 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular)\n",
            "  Downloading botocore-1.35.8-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2,>=1.10->autogluon.core==1.1.1->autogluon.tabular)\n",
            "  Downloading s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.1->autogluon.tabular) (3.1.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.1->autogluon.tabular) (2024.7.4)\n",
            "Downloading autogluon.tabular-1.1.1-py3-none-any.whl (312 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m312.1/312.1 kB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.core-1.1.1-py3-none-any.whl (234 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m234.8/234.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.features-1.1.1-py3-none-any.whl (63 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.4/63.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.common-1.1.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.6/64.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.35.8-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.1/139.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading botocore-1.35.8-py3-none-any.whl (12.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.7/82.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scipy, jmespath, botocore, s3transfer, boto3, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.13.1\n",
            "    Uninstalling scipy-1.13.1:\n",
            "      Successfully uninstalled scipy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "osqp 0.6.7.post0 requires scipy!=1.12.0,>=0.13.2, but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed autogluon.common-1.1.1 autogluon.core-1.1.1 autogluon.features-1.1.1 autogluon.tabular-1.1.1 boto3-1.35.8 botocore-1.35.8 jmespath-1.0.1 s3transfer-0.10.2 scipy-1.12.0\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.10/dist-packages (7.7.1)\n",
            "Collecting ipywidgets\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting comm>=0.1.3 (from ipywidgets)\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets)\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets) (3.0.13)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (71.0.4)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets)\n",
            "  Using cached jedi-0.19.1-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (3.0.47)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (2.16.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets) (0.2.13)\n",
            "Downloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached jedi-0.19.1-py2.py3-none-any.whl (1.6 MB)\n",
            "Installing collected packages: widgetsnbextension, jedi, comm, ipywidgets\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.8\n",
            "    Uninstalling widgetsnbextension-3.6.8:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.8\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed comm-0.2.2 ipywidgets-8.1.5 jedi-0.19.1 widgetsnbextension-4.0.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:53:40.364401Z",
          "iopub.execute_input": "2024-08-13T13:53:40.365091Z",
          "iopub.status.idle": "2024-08-13T13:53:40.409278Z",
          "shell.execute_reply.started": "2024-08-13T13:53:40.365036Z",
          "shell.execute_reply": "2024-08-13T13:53:40.406349Z"
        },
        "trusted": true,
        "id": "2M6uJ2wwIC60"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(\"/kaggle/input/playground-series-s4e8/train.csv\", index_col='id')\n",
        "# orig_df = pd.read_csv(\"/kaggle/input/secondary-mushroom-dataset-data-set/MushroomDataset/secondary_data.csv\", sep=\";\")\n",
        "test_df = pd.read_csv(\"/kaggle/input/playground-series-s4e8/test.csv\", index_col='id')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:53:40.41074Z",
          "iopub.status.idle": "2024-08-13T13:53:40.411404Z",
          "shell.execute_reply.started": "2024-08-13T13:53:40.411143Z",
          "shell.execute_reply": "2024-08-13T13:53:40.411167Z"
        },
        "trusted": true,
        "id": "d7ELWaocIC61"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.duplicated().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-vuTv6eJJuY",
        "outputId": "a13e115d-c92f-41b2-e700-f4e7d0839208"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.drop_duplicates(inplace=True)"
      ],
      "metadata": {
        "id": "tgyq0No2JLuI"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = 'class'"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T12:11:28.261989Z",
          "iopub.execute_input": "2024-08-20T12:11:28.262438Z",
          "iopub.status.idle": "2024-08-20T12:11:28.267933Z",
          "shell.execute_reply.started": "2024-08-20T12:11:28.262398Z",
          "shell.execute_reply": "2024-08-20T12:11:28.266978Z"
        },
        "trusted": true,
        "id": "L1fbcJJYIuAl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = train_df.drop(target, axis=1).columns.to_list()\n",
        "features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T12:11:28.269282Z",
          "iopub.execute_input": "2024-08-20T12:11:28.269789Z",
          "iopub.status.idle": "2024-08-20T12:11:29.017621Z",
          "shell.execute_reply.started": "2024-08-20T12:11:28.269762Z",
          "shell.execute_reply": "2024-08-20T12:11:29.01672Z"
        },
        "trusted": true,
        "id": "UNX54YMtIuAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef688d9b-c9ee-4a8c-93db-c0a3c87616ed"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cap-diameter',\n",
              " 'cap-shape',\n",
              " 'cap-surface',\n",
              " 'cap-color',\n",
              " 'does-bruise-or-bleed',\n",
              " 'gill-attachment',\n",
              " 'gill-spacing',\n",
              " 'gill-color',\n",
              " 'stem-height',\n",
              " 'stem-width',\n",
              " 'stem-root',\n",
              " 'stem-surface',\n",
              " 'stem-color',\n",
              " 'veil-type',\n",
              " 'veil-color',\n",
              " 'has-ring',\n",
              " 'ring-type',\n",
              " 'spore-print-color',\n",
              " 'habitat',\n",
              " 'season']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features_with_high_null_values = [feature for feature in features if (train_df[feature].isna().sum()/len(train_df)*100)>20]\n",
        "features_with_high_null_values"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T12:11:29.018892Z",
          "iopub.execute_input": "2024-08-20T12:11:29.019234Z",
          "iopub.status.idle": "2024-08-20T12:11:32.939179Z",
          "shell.execute_reply.started": "2024-08-20T12:11:29.019202Z",
          "shell.execute_reply": "2024-08-20T12:11:32.9383Z"
        },
        "trusted": true,
        "id": "shVLE3TKIuAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f7b9b56-7596-465d-e85b-d20836d98197"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cap-surface',\n",
              " 'gill-spacing',\n",
              " 'stem-root',\n",
              " 'stem-surface',\n",
              " 'veil-type',\n",
              " 'veil-color',\n",
              " 'spore-print-color']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_features = train_df[features].select_dtypes(include='object').columns.to_list()\n",
        "categorical_features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T12:11:32.940182Z",
          "iopub.execute_input": "2024-08-20T12:11:32.940443Z",
          "iopub.status.idle": "2024-08-20T12:11:35.934008Z",
          "shell.execute_reply.started": "2024-08-20T12:11:32.940419Z",
          "shell.execute_reply": "2024-08-20T12:11:35.933052Z"
        },
        "trusted": true,
        "id": "DOPEu4caIuAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24488a99-88cc-43ad-8a4e-f614e6e04443"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cap-shape',\n",
              " 'cap-surface',\n",
              " 'cap-color',\n",
              " 'does-bruise-or-bleed',\n",
              " 'gill-attachment',\n",
              " 'gill-spacing',\n",
              " 'gill-color',\n",
              " 'stem-root',\n",
              " 'stem-surface',\n",
              " 'stem-color',\n",
              " 'veil-type',\n",
              " 'veil-color',\n",
              " 'has-ring',\n",
              " 'ring-type',\n",
              " 'spore-print-color',\n",
              " 'habitat',\n",
              " 'season']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_features = list(set(features) - set(categorical_features))\n",
        "numerical_features"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-20T12:11:35.935239Z",
          "iopub.execute_input": "2024-08-20T12:11:35.935532Z",
          "iopub.status.idle": "2024-08-20T12:11:35.941629Z",
          "shell.execute_reply.started": "2024-08-20T12:11:35.935507Z",
          "shell.execute_reply": "2024-08-20T12:11:35.940772Z"
        },
        "trusted": true,
        "id": "OCPyuaF4IuAl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140b4a16-93b2-494e-ec8f-3d76ca0ddc9e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['stem-height', 'cap-diameter', 'stem-width']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def cleaner(df):\n",
        "    for col in categorical_features:\n",
        "        df[col] = df[col].fillna('missing')\n",
        "        df.loc[df[col].value_counts(dropna=False)[df[col]].values < 100, col] = \"noise\"\n",
        "        df[col] = df[col].astype('category')\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "BioYkTDVJU1K"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = cleaner(train_df)\n",
        "test_df = cleaner(test_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-17T10:22:02.153672Z",
          "iopub.execute_input": "2024-08-17T10:22:02.154015Z",
          "iopub.status.idle": "2024-08-17T10:22:31.155272Z",
          "shell.execute_reply.started": "2024-08-17T10:22:02.153988Z",
          "shell.execute_reply": "2024-08-17T10:22:31.154481Z"
        },
        "trusted": true,
        "id": "_WPAmB87IuAl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap_diameter_mean = pd.concat([train_df['cap-diameter'], test_df['cap-diameter']]).mean(numeric_only=True)\n",
        "train_df['cap-diameter'].fillna(cap_diameter_mean, inplace=True)\n",
        "test_df['cap-diameter'].fillna(cap_diameter_mean, inplace=True)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-17T10:22:31.157144Z",
          "iopub.execute_input": "2024-08-17T10:22:31.158008Z",
          "iopub.status.idle": "2024-08-17T10:22:31.220402Z",
          "shell.execute_reply.started": "2024-08-17T10:22:31.157972Z",
          "shell.execute_reply": "2024-08-17T10:22:31.219517Z"
        },
        "trusted": true,
        "id": "vZI9YG_pIuAl"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor = TabularPredictor(label='class',\n",
        "                            eval_metric='mcc',\n",
        "                            problem_type='binary').fit(train_df,\n",
        "                                                       presets='best_quality',\n",
        "                                                        time_limit=3600*10,\n",
        "                                                       verbosity=2,\n",
        "                                                       excluded_model_types=['KNN'],\n",
        "                                                       ag_args_fit={'num_gpus': 1}\n",
        "                                                      )\n",
        "results = predictor.fit_summary()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:53:40.419546Z",
          "iopub.status.idle": "2024-08-13T13:53:40.420042Z",
          "shell.execute_reply.started": "2024-08-13T13:53:40.419823Z",
          "shell.execute_reply": "2024-08-13T13:53:40.419843Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RzMfVUqjIC61",
        "outputId": "01bb28d5-0515-4e45-8550-c9b28f9d8f3a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No path specified. Models will be saved in: \"AutogluonModels/ag-20240829_123944\"\n",
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.1.1\n",
            "Python Version:     3.10.12\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Thu Jun 27 21:05:47 UTC 2024\n",
            "CPU Count:          12\n",
            "Memory Avail:       79.65 GB / 83.48 GB (95.4%)\n",
            "Disk Space Avail:   167.65 GB / 201.23 GB (83.3%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 9000s of the 36000s of remaining time (25%).\n",
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2024-08-29 12:39:47,576\tINFO worker.py:1752 -- Started a local Ray instance.\n",
            "\t\tContext path: \"AutogluonModels/ag-20240829_123944/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Beginning AutoGluon training ... Time limit = 8996s\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m AutoGluon will save models to \"AutogluonModels/ag-20240829_123944/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Train Data Rows:    2770617\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Train Data Columns: 20\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Label Column:       class\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Problem Type:       binary\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tAvailable Memory:                    80555.24 MB\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTrain Data (Original)  Memory Usage: 108.34 MB (0.1% of available memory)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t4.9s = Fit runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t20 features in original data used to generate 20 features in processed data.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTrain Data (Processed) Memory Usage: 108.34 MB (0.1% of available memory)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Data preprocessing and feature engineering runtime = 5.89s ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting 108 L1 models ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 5992.04s of the 8990.29s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.08%)\n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m \u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m   warnings.warn(msg, FutureWarning)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=7906)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0364905\tvalid_set's mcc: 0.984262\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=10766)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0359562\tvalid_set's mcc: 0.984478\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=13551)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0362517\tvalid_set's mcc: 0.984477\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=16328)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0364248\tvalid_set's mcc: 0.984384\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=19105)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0370471\tvalid_set's mcc: 0.984116\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=21884)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0360824\tvalid_set's mcc: 0.984567\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=24661)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0372925\tvalid_set's mcc: 0.98418\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=27418)\u001b[0m [1000]\tvalid_set's binary_logloss: 0.0365758\tvalid_set's mcc: 0.984454\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.9846\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t5309.44s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t555.75s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 608.7s of the 3606.95s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.08%)\n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=30419)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=30744)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31071)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31400)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=31725)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32054)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32381)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=32706)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.9782\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t543.14s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t35.7s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 57.61s of the 3055.87s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 479.7s to train, which exceeds the maximum time limit of 57.6s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 50.01s of the 3048.26s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 515.0s to train, which exceeds the maximum time limit of 50.0s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 42.05s of the 3040.31s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.09%)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=33363, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m ModuleNotFoundError: No module named 'catboost'\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m During handling of the above exception, another exception occurred:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \u001b[36mray::_ray_fit()\u001b[39m (pid=33363, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 95, in _fit\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     try_import_catboost()\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/common/utils/try_import.py\", line 70, in try_import_catboost\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     raise ImportError()\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m ImportError\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 38.46s of the 3036.72s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 291.7s to train, which exceeds the maximum time limit of 38.5s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesGini_BAG_L1.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 33.38s of the 3031.63s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 311.0s to train, which exceeds the maximum time limit of 33.4s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesEntr_BAG_L1.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 28.12s of the 3026.38s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.81%)\n",
            "\u001b[36m(_ray_fit pid=33719)\u001b[0m Metric mcc is not supported by this model - using log_loss instead\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: XGBoost_BAG_L1 ... Training model for up to 16.22s of the 3014.47s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.43%)\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:06] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=34054)\u001b[0m   warnings.warn(smsg, UserWarning)\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:37] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:20:38] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=34231)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:08] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=34404)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:37] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:37] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:21:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=34577)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:07] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:09] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=34750)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:38] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:22:39] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=34927)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:09] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:10] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=35100)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:39] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m \u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m     E.g. tree_method = \"hist\", device = \"cuda\"\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m /usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [14:23:41] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m Potential solutions:\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m - Use a data structure that matches the device ordinal in the booster.\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m - Set the device for booster before call to inplace_predict.\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m This warning will only be shown once.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.4852\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t240.52s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t11.88s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 599.2s of the 2768.91s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.9846\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t9.71s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.45s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m   warnings.warn(smsg, UserWarning)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35277)\u001b[0m \u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting 108 L2 models ...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 2758.62s of the 2758.45s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.35%)\n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=35737)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=37010)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=38299)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=39588)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=40861)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=42150)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=43431)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=44708)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.9845\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t2407.76s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t199.29s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 320.92s of the 320.75s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.35%)\n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m \n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=46241)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=46378)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=46518)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=46713)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=46908)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=47079)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=47214)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m /usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m This will raise in a future version.\n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m   warnings.warn(msg, FutureWarning)\n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=47345)\u001b[0m Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.9845\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t221.66s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t2.73s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 94.87s of the 94.7s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Reducing model 'n_estimators' from 300 -> 46 due to low time. Expected time usage reduced from 608.7s -> 94.9s...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tNot enough time to generate out-of-fold predictions for model. Estimated time required was 161.2s compared to 65.24s of available time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping RandomForestGini_BAG_L2.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 35.97s of the 35.8s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 936.2s to train, which exceeds the maximum time limit of 36.0s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping RandomForestEntr_BAG_L2.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: CatBoost_BAG_L2 ... Training model for up to 21.95s of the 21.78s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.37%)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=48134, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m ModuleNotFoundError: No module named 'catboost'\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m During handling of the above exception, another exception occurred:\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \u001b[36mray::_ray_fit()\u001b[39m (pid=48134, ip=172.28.0.12)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     out = self._fit(**kwargs)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 95, in _fit\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     try_import_catboost()\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m   File \"/usr/local/lib/python3.10/dist-packages/autogluon/common/utils/try_import.py\", line 70, in try_import_catboost\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m     raise ImportError()\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m ImportError\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 17.9s of the 17.73s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 353.2s to train, which exceeds the maximum time limit of 17.9s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesGini_BAG_L2.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 11.53s of the 11.36s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tWarning: Model is expected to require 280.4s to train, which exceeds the maximum time limit of 11.5s, skipping model...\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tTime limit exceeded... Skipping ExtraTreesEntr_BAG_L2.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 4.73s of remaining time.\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \tEnsemble Weights: {'LightGBMXT_BAG_L1': 0.833, 'LightGBMXT_BAG_L2': 0.167}\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.9846\t = Validation score   (mcc)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t15.09s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m \t0.44s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m AutoGluon training complete, total runtime = 9007.36s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 623.1 rows/s (346328 batch size)\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240829_123944/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=6892)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                 model  score_holdout  score_val eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0      LightGBM_BAG_L2       0.984942   0.984489         mcc       82.507391     606.062334  6314.761749                 0.757015                2.733535         221.661892            2       True          6\n",
            "1  WeightedEnsemble_L3       0.984915   0.984614         mcc      105.693363     803.054405  8515.956946                 0.005618                0.435504          15.093444            3       True          7\n",
            "2    LightGBMXT_BAG_L1       0.984862   0.984624         mcc       64.228401     555.746606  5309.440105                64.228401              555.746606        5309.440105            1       True          1\n",
            "3  WeightedEnsemble_L2       0.984862   0.984624         mcc       64.234153     556.195681  5319.146319                 0.005752                0.449075           9.706214            2       True          4\n",
            "4    LightGBMXT_BAG_L2       0.984728   0.984456         mcc      105.687745     802.618900  8500.863502                23.937369              199.290102        2407.763645            2       True          5\n",
            "5      LightGBM_BAG_L1       0.979043   0.978242         mcc        5.045929      35.704038   543.144552                 5.045929               35.704038         543.144552            1       True          2\n",
            "6       XGBoost_BAG_L1       0.483138   0.485182         mcc       12.476046      11.878155   240.515200                12.476046               11.878155         240.515200            1       True          3\n",
            "\t1\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: False)\n",
            "\t9124s\t = DyStack   runtime |\t26876s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=1.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=1)`\n",
            "Beginning AutoGluon training ... Time limit = 26876s\n",
            "AutoGluon will save models to \"AutogluonModels/ag-20240829_123944\"\n",
            "Train Data Rows:    3116945\n",
            "Train Data Columns: 20\n",
            "Label Column:       class\n",
            "Problem Type:       binary\n",
            "Preprocessing data ...\n",
            "Selected class <--> label mapping:  class 1 = p, class 0 = e\n",
            "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (p) vs negative (e) class.\n",
            "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    81187.34 MB\n",
            "\tTrain Data (Original)  Memory Usage: 121.88 MB (0.2% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
            "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
            "\t\t('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
            "\t5.7s = Fit runtime\n",
            "\t20 features in original data used to generate 20 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 121.88 MB (0.2% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 6.77s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'mcc'\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
            "Fitting 108 L1 models ...\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 17908.24s of the 26869.06s of remaining time.\n",
            "/usr/local/lib/python3.10/dist-packages/dask/dataframe/__init__.py:42: FutureWarning: \n",
            "Dask dataframe query planning is disabled because dask-expr is not installed.\n",
            "\n",
            "You can install it with `pip install dask[dataframe]` or `conda install dask`.\n",
            "This will raise in a future version.\n",
            "\n",
            "  warnings.warn(msg, FutureWarning)\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
            "\t0.9848\t = Validation score   (mcc)\n",
            "\t13715.79s\t = Training   runtime\n",
            "\t1215.85s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 4019.65s of the 12980.48s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.21%)\n",
            "\t0.9842\t = Validation score   (mcc)\n",
            "\t3561.02s\t = Training   runtime\n",
            "\t365.24s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 409.11s of the 9369.94s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 240 due to low time. Expected time usage reduced from 509.4s -> 409.1s...\n",
            "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 519.87s compared to 281.42s of available time.\n",
            "\tTime limit exceeded... Skipping RandomForestGini_BAG_L1.\n",
            "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 157.96s of the 9118.79s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 60 due to low time. Expected time usage reduced from 785.0s -> 158.0s...\n",
            "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 183.21s compared to 126.26s of available time.\n",
            "\tTime limit exceeded... Skipping RandomForestEntr_BAG_L1.\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 78.21s of the 9039.04s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.24%)\n",
            "\tWarning: Exception caused CatBoost_BAG_L1 to fail during training (ImportError)... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=122594, ip=172.28.0.12)\n",
            "ModuleNotFoundError: No module named 'catboost'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[36mray::_ray_fit()\u001b[39m (pid=122594, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 95, in _fit\n",
            "    try_import_catboost()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/common/utils/try_import.py\", line 70, in try_import_catboost\n",
            "    raise ImportError()\n",
            "ImportError\n",
            "Fitting model: ExtraTreesGini_BAG_L1 ... Training model for up to 74.62s of the 9035.44s of remaining time.\n",
            "\tWarning: Reducing model 'n_estimators' from 300 -> 49 due to low time. Expected time usage reduced from 449.0s -> 74.6s...\n",
            "\tNot enough time to generate out-of-fold predictions for model. Estimated time required was 181.87s compared to 58.61s of available time.\n",
            "\tTime limit exceeded... Skipping ExtraTreesGini_BAG_L1.\n",
            "Fitting model: ExtraTreesEntr_BAG_L1 ... Training model for up to 35.56s of the 8996.38s of remaining time.\n",
            "\tWarning: Model is expected to require 427.9s to train, which exceeds the maximum time limit of 35.6s, skipping model...\n",
            "\tTime limit exceeded... Skipping ExtraTreesEntr_BAG_L1.\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 28.69s of the 8989.51s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.06%)\n",
            "\tTime limit exceeded... Skipping NeuralNetFastAI_BAG_L1.\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 19.05s of the 8979.87s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.61%)\n",
            "\t0.485\t = Validation score   (mcc)\n",
            "\t264.73s\t = Training   runtime\n",
            "\t13.8s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 1790.82s of the 8709.89s of remaining time.\n",
            "\tEnsemble Weights: {'LightGBMXT_BAG_L1': 1.0}\n",
            "\t0.9848\t = Validation score   (mcc)\n",
            "\t9.49s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "Excluded models: ['KNN'] (Specified by `excluded_model_types`)\n",
            "Fitting 108 L2 models ...\n",
            "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 8699.84s of the 8699.7s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.53%)\n",
            "\t0.9824\t = Validation score   (mcc)\n",
            "\t1468.45s\t = Training   runtime\n",
            "\t96.17s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L2 ... Training model for up to 7226.72s of the 7226.57s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.53%)\n",
            "\t0.9846\t = Validation score   (mcc)\n",
            "\t376.42s\t = Training   runtime\n",
            "\t5.02s\t = Validation runtime\n",
            "Fitting model: RandomForestGini_BAG_L2 ... Training model for up to 6845.81s of the 6845.67s of remaining time.\n",
            "\t0.9848\t = Validation score   (mcc)\n",
            "\t344.81s\t = Training   runtime\n",
            "\t113.51s\t = Validation runtime\n",
            "Fitting model: RandomForestEntr_BAG_L2 ... Training model for up to 6384.91s of the 6384.77s of remaining time.\n",
            "\t0.9848\t = Validation score   (mcc)\n",
            "\t372.67s\t = Training   runtime\n",
            "\t129.92s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L2 ... Training model for up to 5879.86s of the 5879.72s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.56%)\n",
            "\tWarning: Exception caused CatBoost_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=138056, ip=172.28.0.12)\n",
            "ModuleNotFoundError: No module named 'catboost'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[36mray::_ray_fit()\u001b[39m (pid=138056, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 95, in _fit\n",
            "    try_import_catboost()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/common/utils/try_import.py\", line 70, in try_import_catboost\n",
            "    raise ImportError()\n",
            "ImportError\n",
            "Fitting model: ExtraTreesGini_BAG_L2 ... Training model for up to 5875.3s of the 5875.16s of remaining time.\n",
            "\t0.9849\t = Validation score   (mcc)\n",
            "\t169.04s\t = Training   runtime\n",
            "\t104.94s\t = Validation runtime\n",
            "Fitting model: ExtraTreesEntr_BAG_L2 ... Training model for up to 5598.81s of the 5598.67s of remaining time.\n",
            "\t0.9849\t = Validation score   (mcc)\n",
            "\t184.4s\t = Training   runtime\n",
            "\t118.55s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 5293.37s of the 5293.23s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.61%)\n",
            "\t0.9848\t = Validation score   (mcc)\n",
            "\t3453.94s\t = Training   runtime\n",
            "\t29.0s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L2 ... Training model for up to 1831.45s of the 1831.3s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=2.03%)\n",
            "\t0.9847\t = Validation score   (mcc)\n",
            "\t312.56s\t = Training   runtime\n",
            "\t14.88s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L2 ... Training model for up to 1512.85s of the 1512.7s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.44%)\n",
            "\t0.9848\t = Validation score   (mcc)\n",
            "\t1262.57s\t = Training   runtime\n",
            "\t25.28s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 243.03s of the 242.89s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.58%)\n",
            "\t0.9845\t = Validation score   (mcc)\n",
            "\t216.77s\t = Training   runtime\n",
            "\t3.49s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L2 ... Training model for up to 21.7s of the 21.55s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.57%)\n",
            "\tWarning: Exception caused CatBoost_r177_BAG_L2 to fail during training (ImportError)... Skipping this model.\n",
            "\t\t\u001b[36mray::_ray_fit()\u001b[39m (pid=164956, ip=172.28.0.12)\n",
            "ModuleNotFoundError: No module named 'catboost'\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "\u001b[36mray::_ray_fit()\u001b[39m (pid=164956, ip=172.28.0.12)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/ensemble/fold_fitting_strategy.py\", line 402, in _ray_fit\n",
            "    fold_model.fit(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, time_limit=time_limit_fold, **resources, **kwargs_fold)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/core/models/abstract/abstract_model.py\", line 856, in fit\n",
            "    out = self._fit(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/tabular/models/catboost/catboost_model.py\", line 95, in _fit\n",
            "    try_import_catboost()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/autogluon/common/utils/try_import.py\", line 70, in try_import_catboost\n",
            "    raise ImportError()\n",
            "ImportError\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L2 ... Training model for up to 17.18s of the 17.04s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (1.0 workers, per: cpus=1, gpus=1, memory=1.44%)\n",
            "\tTime limit exceeded... Skipping NeuralNetTorch_r79_BAG_L2.\n",
            "Fitting model: WeightedEnsemble_L3 ... Training model for up to 869.98s of the -19.23s of remaining time.\n",
            "\tEnsemble Weights: {'ExtraTreesGini_BAG_L2': 0.353, 'ExtraTreesEntr_BAG_L2': 0.294, 'RandomForestEntr_BAG_L2': 0.118, 'LightGBMLarge_BAG_L2': 0.118, 'LightGBMXT_BAG_L2': 0.059, 'RandomForestGini_BAG_L2': 0.059}\n",
            "\t0.9849\t = Validation score   (mcc)\n",
            "\t39.04s\t = Training   runtime\n",
            "\t0.42s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 26934.97s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 222.3 rows/s (389619 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20240829_123944\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Summary of fit() ***\n",
            "Estimated performance of each model:\n",
            "                      model  score_val eval_metric  pred_time_val      fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0       WeightedEnsemble_L3   0.984879         mcc    2161.911935  20336.727506                0.421159          39.043306            3       True         15\n",
            "1     ExtraTreesGini_BAG_L2   0.984869         mcc    1699.843977  17710.582196              104.941682         169.041391            2       True          9\n",
            "2     ExtraTreesEntr_BAG_L2   0.984858         mcc    1713.449089  17725.945582              118.546794         184.404777            2       True         10\n",
            "3   RandomForestEntr_BAG_L2   0.984825         mcc    1724.826225  17914.208343              129.923930         372.667537            2       True          8\n",
            "4   RandomForestGini_BAG_L2   0.984816         mcc    1708.412703  17886.353657              113.510408         344.812852            2       True          7\n",
            "5    NeuralNetFastAI_BAG_L2   0.984796         mcc    1623.905856  20995.484794               29.003561        3453.943989            2       True         11\n",
            "6         LightGBMXT_BAG_L1   0.984788         mcc    1215.853803  13715.794357             1215.853803       13715.794357            1       True          1\n",
            "7       WeightedEnsemble_L2   0.984788         mcc    1216.276582  13725.287068                0.422779           9.492712            2       True          4\n",
            "8     NeuralNetTorch_BAG_L2   0.984786         mcc    1620.186430  18804.115685               25.284135        1262.574880            2       True         13\n",
            "9            XGBoost_BAG_L2   0.984691         mcc    1609.783762  17854.097932               14.881467         312.557127            2       True         12\n",
            "10          LightGBM_BAG_L2   0.984591         mcc    1599.923386  17917.962235                5.021091         376.421430            2       True          6\n",
            "11     LightGBMLarge_BAG_L2   0.984534         mcc    1598.395952  17758.306601                3.493657         216.765796            2       True         14\n",
            "12          LightGBM_BAG_L1   0.984248         mcc     365.244216   3561.017158              365.244216        3561.017158            1       True          2\n",
            "13        LightGBMXT_BAG_L2   0.982411         mcc    1691.074306  19009.991848               96.172011        1468.451042            2       True          5\n",
            "14           XGBoost_BAG_L1   0.485010         mcc      13.804276    264.729290               13.804276         264.729290            1       True          3\n",
            "Number of models trained: 15\n",
            "Types of models trained:\n",
            "{'WeightedEnsembleModel', 'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_RF', 'StackerEnsembleModel_XT', 'StackerEnsembleModel_TabularNeuralNetTorch'}\n",
            "Bagging used: True  (with 8 folds)\n",
            "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
            "Feature Metadata (Processed):\n",
            "(raw dtype, special dtypes):\n",
            "('category', []) : 17 | ['cap-shape', 'cap-surface', 'cap-color', 'does-bruise-or-bleed', 'gill-attachment', ...]\n",
            "('float', [])    :  3 | ['cap-diameter', 'stem-height', 'stem-width']\n",
            "Plot summary of models saved to file: AutogluonModels/ag-20240829_123944SummaryOfModels.html\n",
            "*** End of fit() summary ***\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.leaderboard()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:53:40.422875Z",
          "iopub.status.idle": "2024-08-13T13:53:40.423361Z",
          "shell.execute_reply.started": "2024-08-13T13:53:40.423129Z",
          "shell.execute_reply": "2024-08-13T13:53:40.423148Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        },
        "id": "v2AV8-AYIC61",
        "outputId": "7cf8913b-11c8-489a-aaa3-d80136a34b4b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                      model  score_val eval_metric  pred_time_val  \\\n",
              "0       WeightedEnsemble_L3   0.984879         mcc    2161.911935   \n",
              "1     ExtraTreesGini_BAG_L2   0.984869         mcc    1699.843977   \n",
              "2     ExtraTreesEntr_BAG_L2   0.984858         mcc    1713.449089   \n",
              "3   RandomForestEntr_BAG_L2   0.984825         mcc    1724.826225   \n",
              "4   RandomForestGini_BAG_L2   0.984816         mcc    1708.412703   \n",
              "5    NeuralNetFastAI_BAG_L2   0.984796         mcc    1623.905856   \n",
              "6         LightGBMXT_BAG_L1   0.984788         mcc    1215.853803   \n",
              "7       WeightedEnsemble_L2   0.984788         mcc    1216.276582   \n",
              "8     NeuralNetTorch_BAG_L2   0.984786         mcc    1620.186430   \n",
              "9            XGBoost_BAG_L2   0.984691         mcc    1609.783762   \n",
              "10          LightGBM_BAG_L2   0.984591         mcc    1599.923386   \n",
              "11     LightGBMLarge_BAG_L2   0.984534         mcc    1598.395952   \n",
              "12          LightGBM_BAG_L1   0.984248         mcc     365.244216   \n",
              "13        LightGBMXT_BAG_L2   0.982411         mcc    1691.074306   \n",
              "14           XGBoost_BAG_L1   0.485010         mcc      13.804276   \n",
              "\n",
              "        fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
              "0   20336.727506                0.421159          39.043306            3   \n",
              "1   17710.582196              104.941682         169.041391            2   \n",
              "2   17725.945582              118.546794         184.404777            2   \n",
              "3   17914.208343              129.923930         372.667537            2   \n",
              "4   17886.353657              113.510408         344.812852            2   \n",
              "5   20995.484794               29.003561        3453.943989            2   \n",
              "6   13715.794357             1215.853803       13715.794357            1   \n",
              "7   13725.287068                0.422779           9.492712            2   \n",
              "8   18804.115685               25.284135        1262.574880            2   \n",
              "9   17854.097932               14.881467         312.557127            2   \n",
              "10  17917.962235                5.021091         376.421430            2   \n",
              "11  17758.306601                3.493657         216.765796            2   \n",
              "12   3561.017158              365.244216        3561.017158            1   \n",
              "13  19009.991848               96.172011        1468.451042            2   \n",
              "14    264.729290               13.804276         264.729290            1   \n",
              "\n",
              "    can_infer  fit_order  \n",
              "0        True         15  \n",
              "1        True          9  \n",
              "2        True         10  \n",
              "3        True          8  \n",
              "4        True          7  \n",
              "5        True         11  \n",
              "6        True          1  \n",
              "7        True          4  \n",
              "8        True         13  \n",
              "9        True         12  \n",
              "10       True          6  \n",
              "11       True         14  \n",
              "12       True          2  \n",
              "13       True          5  \n",
              "14       True          3  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f976ddcd-38ef-47e7-a392-a2dc49f6c536\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model</th>\n",
              "      <th>score_val</th>\n",
              "      <th>eval_metric</th>\n",
              "      <th>pred_time_val</th>\n",
              "      <th>fit_time</th>\n",
              "      <th>pred_time_val_marginal</th>\n",
              "      <th>fit_time_marginal</th>\n",
              "      <th>stack_level</th>\n",
              "      <th>can_infer</th>\n",
              "      <th>fit_order</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>WeightedEnsemble_L3</td>\n",
              "      <td>0.984879</td>\n",
              "      <td>mcc</td>\n",
              "      <td>2161.911935</td>\n",
              "      <td>20336.727506</td>\n",
              "      <td>0.421159</td>\n",
              "      <td>39.043306</td>\n",
              "      <td>3</td>\n",
              "      <td>True</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ExtraTreesGini_BAG_L2</td>\n",
              "      <td>0.984869</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1699.843977</td>\n",
              "      <td>17710.582196</td>\n",
              "      <td>104.941682</td>\n",
              "      <td>169.041391</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ExtraTreesEntr_BAG_L2</td>\n",
              "      <td>0.984858</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1713.449089</td>\n",
              "      <td>17725.945582</td>\n",
              "      <td>118.546794</td>\n",
              "      <td>184.404777</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RandomForestEntr_BAG_L2</td>\n",
              "      <td>0.984825</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1724.826225</td>\n",
              "      <td>17914.208343</td>\n",
              "      <td>129.923930</td>\n",
              "      <td>372.667537</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>RandomForestGini_BAG_L2</td>\n",
              "      <td>0.984816</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1708.412703</td>\n",
              "      <td>17886.353657</td>\n",
              "      <td>113.510408</td>\n",
              "      <td>344.812852</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>NeuralNetFastAI_BAG_L2</td>\n",
              "      <td>0.984796</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1623.905856</td>\n",
              "      <td>20995.484794</td>\n",
              "      <td>29.003561</td>\n",
              "      <td>3453.943989</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>LightGBMXT_BAG_L1</td>\n",
              "      <td>0.984788</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1215.853803</td>\n",
              "      <td>13715.794357</td>\n",
              "      <td>1215.853803</td>\n",
              "      <td>13715.794357</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>WeightedEnsemble_L2</td>\n",
              "      <td>0.984788</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1216.276582</td>\n",
              "      <td>13725.287068</td>\n",
              "      <td>0.422779</td>\n",
              "      <td>9.492712</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>NeuralNetTorch_BAG_L2</td>\n",
              "      <td>0.984786</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1620.186430</td>\n",
              "      <td>18804.115685</td>\n",
              "      <td>25.284135</td>\n",
              "      <td>1262.574880</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>XGBoost_BAG_L2</td>\n",
              "      <td>0.984691</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1609.783762</td>\n",
              "      <td>17854.097932</td>\n",
              "      <td>14.881467</td>\n",
              "      <td>312.557127</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>LightGBM_BAG_L2</td>\n",
              "      <td>0.984591</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1599.923386</td>\n",
              "      <td>17917.962235</td>\n",
              "      <td>5.021091</td>\n",
              "      <td>376.421430</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>LightGBMLarge_BAG_L2</td>\n",
              "      <td>0.984534</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1598.395952</td>\n",
              "      <td>17758.306601</td>\n",
              "      <td>3.493657</td>\n",
              "      <td>216.765796</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>14</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>LightGBM_BAG_L1</td>\n",
              "      <td>0.984248</td>\n",
              "      <td>mcc</td>\n",
              "      <td>365.244216</td>\n",
              "      <td>3561.017158</td>\n",
              "      <td>365.244216</td>\n",
              "      <td>3561.017158</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>LightGBMXT_BAG_L2</td>\n",
              "      <td>0.982411</td>\n",
              "      <td>mcc</td>\n",
              "      <td>1691.074306</td>\n",
              "      <td>19009.991848</td>\n",
              "      <td>96.172011</td>\n",
              "      <td>1468.451042</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>XGBoost_BAG_L1</td>\n",
              "      <td>0.485010</td>\n",
              "      <td>mcc</td>\n",
              "      <td>13.804276</td>\n",
              "      <td>264.729290</td>\n",
              "      <td>13.804276</td>\n",
              "      <td>264.729290</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f976ddcd-38ef-47e7-a392-a2dc49f6c536')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f976ddcd-38ef-47e7-a392-a2dc49f6c536 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f976ddcd-38ef-47e7-a392-a2dc49f6c536');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-058d0e6b-aed3-448b-a9f4-377b541e349c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-058d0e6b-aed3-448b-a9f4-377b541e349c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-058d0e6b-aed3-448b-a9f4-377b541e349c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"predictor\",\n  \"rows\": 15,\n  \"fields\": [\n    {\n      \"column\": \"model\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 15,\n        \"samples\": [\n          \"XGBoost_BAG_L2\",\n          \"LightGBMLarge_BAG_L2\",\n          \"WeightedEnsemble_L3\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"score_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12898540294184904,\n        \"min\": 0.4850102563271241,\n        \"max\": 0.9848789628258141,\n        \"num_unique_values\": 14,\n        \"samples\": [\n          0.9845909102322004,\n          0.9842477510357854,\n          0.9848789628258141\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"eval_metric\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"mcc\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_time_val\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 555.1560742725962,\n        \"min\": 13.804275751113892,\n        \"max\": 2161.9119353294373,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          1609.7837615013123\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fit_time\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5947.334982958815,\n        \"min\": 264.7292902469635,\n        \"max\": 20995.484793901443,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          17854.097932338715\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"pred_time_val_marginal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 309.86684808947626,\n        \"min\": 0.4211587905883789,\n        \"max\": 1215.8538026809692,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          14.881466627120972\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fit_time_marginal\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3514.7890858809533,\n        \"min\": 9.492711544036865,\n        \"max\": 13715.794356584549,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          312.55712723731995\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"stack_level\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"can_infer\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"fit_order\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 4,\n        \"min\": 1,\n        \"max\": 15,\n        \"num_unique_values\": 15,\n        \"samples\": [\n          12\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = predictor.predict(test_df)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:53:40.425736Z",
          "iopub.status.idle": "2024-08-13T13:53:40.426451Z",
          "shell.execute_reply.started": "2024-08-13T13:53:40.426102Z",
          "shell.execute_reply": "2024-08-13T13:53:40.426131Z"
        },
        "trusted": true,
        "id": "dyDZ5_bCIC62"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prop = predictor.predict_proba(test_df)"
      ],
      "metadata": {
        "id": "Mj4ZV761wor5"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_prop.to_csv('submission_autogluon_pre_prop.csv', index=False)"
      ],
      "metadata": {
        "id": "k2x1FUhKwsHs"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub = pd.read_csv('/kaggle/input/playground-series-s4e8/sample_submission.csv')\n",
        "sub['class'] = y_pred.to_list() # 저장이 안 돼서 리스트로 변환 후 저장\n",
        "sub.to_csv('submission_autogluon_pre.csv', index=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-08-13T13:53:40.429329Z",
          "iopub.status.idle": "2024-08-13T13:53:40.430014Z",
          "shell.execute_reply.started": "2024-08-13T13:53:40.429678Z",
          "shell.execute_reply": "2024-08-13T13:53:40.429721Z"
        },
        "trusted": true,
        "id": "mqgolme1IC62"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "u69UvRwvtXi9",
        "outputId": "aa3329b5-66dd-4dcc-caf6-0021a2145e56"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        id class\n",
              "0  3116945     e\n",
              "1  3116946     p\n",
              "2  3116947     p\n",
              "3  3116948     p\n",
              "4  3116949     e"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b3476de7-b0e0-4b19-8000-4ee4d6508c34\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3116945</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>3116946</td>\n",
              "      <td>p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3116947</td>\n",
              "      <td>p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3116948</td>\n",
              "      <td>p</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>3116949</td>\n",
              "      <td>e</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b3476de7-b0e0-4b19-8000-4ee4d6508c34')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b3476de7-b0e0-4b19-8000-4ee4d6508c34 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b3476de7-b0e0-4b19-8000-4ee4d6508c34');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-36d082e7-9946-4fe1-8caf-26444870f58d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-36d082e7-9946-4fe1-8caf-26444870f58d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-36d082e7-9946-4fe1-8caf-26444870f58d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "sub"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}